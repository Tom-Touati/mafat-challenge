{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fresh Idea\n",
    "## separate one/zero activity of domains\n",
    "- replace zeros by minus one\n",
    "- calculate the class activity for 3 hours bins for each domain\n",
    "- calculate the user activity for gaussian around center of 3 hour bins\n",
    "- calculate the likelihood of the person being a 1/-1 in that time \n",
    "- add user general metrics including domain cls activity and usage patterns\n",
    "\n",
    "### questions\n",
    "- how to take into account times when the person used a website when others didnt?\n",
    "- how to give likelihood when the person didn't show any nearby activity?\n",
    "- what if he used a similar website at same time but more nich? \n",
    "- how to average the bins weighted by the significance of that bin?\n",
    "- how to give weight to the magnitude of number of users entering? probability of 1 with confidence\n",
    "- what about sparse websites?\n",
    "- how to not let times where there are no activity take a lot of weight?\n",
    "### enhancements\n",
    "- create graph embedding of urls\n",
    "- for each bin, calculate the metric per url\n",
    "- instead of only looking at the specific website, take into account websites with similar usages,\n",
    "  for example looking at same domain_cls usage in gaussian around bin, or looking at domain embeddings and looking at the activity in similar embeddings weighted by the distance in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export MODIN_CPUS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import freeze_support\n",
    "# os.environ[\"MODIN_CPUS\"] = \"2\"\n",
    "os.environ[\"RAY_OBJECT_STORE_MEMORY\"] = str(5 * (1024 ** 3))\n",
    "from modin.config import NPartitions\n",
    "NPartitions.put(12)\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "\n",
    "import ray\n",
    "\n",
    "\n",
    "# Set a higher number of partitions to reduce memory per partition\n",
    "# ctx = ray.init()#object_store_memory=24000000000)#,include_dashboard=True)  # Object store memory (~25GB)\n",
    "#                 redis_max_memory=10000000000)  # Redis memory (~10GB)\n",
    "\n",
    "\n",
    "import modin.pandas as mpd\n",
    "# print(ctx.dashboard_url)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# ray.init()\n",
    "# NPartitions.put(16)\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        # First get 1000 random Device_IDs\n",
    "        selective_device_ids_query = \"\"\"\n",
    "        WITH random_devices AS (\n",
    "            SELECT DISTINCT Device_ID \n",
    "            FROM data \n",
    "            LIMIT 1000\n",
    "        )\n",
    "        SELECT * \n",
    "        FROM data \n",
    "        WHERE Device_ID IN (SELECT Device_ID FROM random_devices)\n",
    "        \"\"\"\n",
    "        device_ids_query = \"\"\"SELECT * from data\"\"\"\n",
    "        df = mpd.read_sql(selective_device_ids_query, con)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "dbfile = '../train_data_for_competition/training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())\n",
    "del conn  # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = db_df._repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"23-04 to 18-05\"\n",
    "db_df = db_df[db_df[\"Domain_Name\"]!=1732927] # remove empty url\n",
    "db_df[\"Datetime\"] = mpd.to_datetime(db_df[\"Datetime\"])\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x-x[\"Datetime\"].min()).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df.set_index(\"Datetime\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_devices(device_target_df, test_size=0.2, random_state=42):    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    return train_device_ids, test_device_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = db_df.groupby(\"Device_ID\").first().reset_index()\n",
    "train_devices, test_device_ids = get_train_test_devices(devices)\n",
    "train_db_df = db_df[db_df[\"Device_ID\"].isin(train_devices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_per_domain = train_db_df.groupby(\"Domain_Name\")[\"Device_ID\"].nunique()\n",
    "domains = devices_per_domain[devices_per_domain>10].index\n",
    "train_db_df = train_db_df[train_db_df[\"Domain_Name\"].isin(domains)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess timeseries\n",
    "def process_activity_timeseries(domain_df,bin_hours=6,gaussian_filter=True,n_days_each_side=3,std=1.5,drop_na=True,drop_zeros=False):\n",
    "    activity_per_3h = domain_df[\"Device_ID\"].resample(f'{str(bin_hours)}h').nunique()\n",
    "    gaussian_window_hours = int(n_days_each_side*24/bin_hours*2) # n_days_each_side * 24h / 3h_per_bin * 2 sides\n",
    "    if gaussian_filter:\n",
    "        activity_per_3h = activity_per_3h.rolling(window=gaussian_window_hours, win_type='gaussian',center=True,min_periods=1,closed=\"both\").mean(std=std)\n",
    "    if drop_na:\n",
    "        activity_per_3h.dropna(inplace=True)\n",
    "    if drop_zeros:\n",
    "        activity_per_3h = activity_per_3h[activity_per_3h!=0]\n",
    "    activity_per_3h.rename(\"Activity\",inplace=True)\n",
    "    return activity_per_3h.round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "process_domain_timeseries = partial(process_activity_timeseries,gaussian_filter=True,n_days_each_side=3,std=1.5,drop_na=True,drop_zeros=False)\n",
    "process_domain_timeseries.__name__ =process_activity_timeseries.__name__\n",
    "domain_time_series = train_db_df.groupby([\"Domain_Name\",\"Target\"]).apply(process_domain_timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_domain_time_series = db_df.groupby([\"Device_ID\",\"Domain_Name\"]).apply(process_activity_timeseries).swaplevel(0,1)\n",
    "\n",
    "# Reset index to get all columns as regular columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_days_per_user(user_domain_ts):\n",
    "    \"\"\"\n",
    "    Calculate the number of unique days each user had any activity.\n",
    "    \n",
    "    Args:\n",
    "        user_domain_ts: MultiIndex Series with levels [Domain_Name, Device_ID, Datetime]\n",
    "        \n",
    "    Returns:\n",
    "        Series with index Device_ID and values being number of unique active days\n",
    "    \"\"\"\n",
    "    # Reset index to get Datetime as a column\n",
    "    df = user_domain_ts.reset_index()\n",
    "    \n",
    "    # Convert Datetime to date (removing time component)\n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "    \n",
    "    # Group by Device_ID and count unique dates where Activity > 0\n",
    "    active_days = df[df['Activity'] > 0].groupby('Device_ID')['Date'].nunique()\n",
    "    \n",
    "    return active_days.astype(int)\n",
    "daily_active_days = get_active_days_per_user(user_domain_time_series)\n",
    "daily_active_days.name = \"Active_Days\"\n",
    "daily_active_days = (daily_active_days-daily_active_days.min())/(daily_active_days.max()-daily_active_days.min())*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate domain fractions and activity separately\n",
    "# First, calculate activity fraction\n",
    "domain_fraction_ts = domain_time_series.copy()\n",
    "del domain_time_series\n",
    "domain_fraction_ts[\"activity_fraction\"] = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"]).transform(lambda x: x/x.sum())\n",
    "# Add the sum of activity as a new column\n",
    "domain_activity = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"])[[\"Activity\"]].sum()\n",
    "\n",
    "domain_activity = domain_activity.rename(columns={\"Activity\": \"target_domain_activity\"})\n",
    "# Merge the results\n",
    "domain_fraction_ts = domain_fraction_ts.merge(domain_activity, left_index=True, right_index=True)\n",
    "# Reset index to get Target as a column, then pivot to get Target as columns\n",
    "pivot_fraction_ts = domain_fraction_ts.reset_index().pivot(\n",
    "    index=['Datetime', 'Domain_Name'],\n",
    "    columns='Target'\n",
    ").fillna(0)\n",
    "pivot_fraction_ts.columns = [f'{col[0]}_{col[1]}' for col in pivot_fraction_ts.columns]\n",
    "# Rename columns for clarity\n",
    "# pivot_fraction_ts.columns = ['activity_0', 'activity_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pivot_fraction_ts.merge(user_domain_time_series.reset_index(),how=\"left\",on=[\"Domain_Name\",\"Datetime\"],)\n",
    "merged_df.set_index([\"Datetime\",\"Domain_Name\",\"Device_ID\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_domain_time_series\n",
    "del pivot_fraction_ts\n",
    "del domain_fraction_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def class_probability_score(active, p_active_given_a, p_active_given_b, prior_a=0.5, total_users=100):\n",
    "    \"\"\"\n",
    "    Calculate class probability score with vectorized operations\n",
    "    \n",
    "    Args:\n",
    "        active: Boolean indicating if user was active\n",
    "        p_active_given_a: Probability of activity given class A (0)\n",
    "        p_active_given_b: Probability of activity given class B (1)\n",
    "        prior_a: Prior probability for class A\n",
    "        total_users: Total number of users for confidence calculation\n",
    "    \"\"\"\n",
    "    # Use numpy for vectorized operations\n",
    "    likelihood_a = np.where(active, p_active_given_a, 1 - p_active_given_a)\n",
    "    likelihood_b = np.where(active, p_active_given_b, 1 - p_active_given_b)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    evidence = (likelihood_a * prior_a + likelihood_b * (1 - prior_a))\n",
    "    posterior_a = (likelihood_a * prior_a) / evidence\n",
    "    \n",
    "    # Simplified confidence calculation\n",
    "    # alpha = 1 + posterior_a * total_users\n",
    "    # beta = 1 + (1 - posterior_a) * total_users\n",
    "    # ci_width = stats.beta.interval(0.95, alpha, beta)[1] - stats.beta.interval(0.95, alpha, beta)[0]\n",
    "    \n",
    "    # # Calculate final score\n",
    "    # raw_score = 2 * posterior_a - 1\n",
    "    # confidence_factor = 1 - ci_width\n",
    "    \n",
    "    return posterior_a #* confidence_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"bin_activity\"] = merged_df[\"Activity_0\"]+merged_df[\"Activity_1\"]\n",
    "merged_df[\"total_activity\"] = (merged_df[\"target_domain_activity_0\"]+merged_df[\"target_domain_activity_1\"])\n",
    "merged_df[\"relative_0_activity\"] = merged_df[\"target_domain_activity_0\"]/merged_df[\"total_activity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "merged_df[\"score\"]=class_probability_score(merged_df[\"Activity\"], merged_df[\"activity_fraction_0\"], merged_df[\"activity_fraction_1\"], prior_a=merged_df[\"relative_0_activity\"], total_users=merged_df[\"bin_activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"weighted_score\"] = (merged_df[\"score\"])*(merged_df[\"bin_activity\"])#np.log(1+merged_df[\"bin_activity\"]).astype(int))\n",
    "final_scores = merged_df.groupby([\"Device_ID\",\"Domain_Name\"])[\"weighted_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot = final_scores.to_frame().reset_index().pivot(index=\"Device_ID\",columns=\"Domain_Name\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_per_time_bin(df,bin_hours=3):\n",
    "    # Convert datetime to time only\n",
    "    # time_index = db_df.index.to_series().dt.time\n",
    "    # df[\"time\"] = time_index\n",
    "    df_copy = db_df.copy()\n",
    "    df_copy[\"time\"] = db_df.index.to_series().dt.hour.astype(int)//bin_hours\n",
    "    df_copy[\"day_part_activity\"] = 0\n",
    "    activity_per_3h = df_copy[[\"Device_ID\",\"time\",\"day_part_activity\"]].groupby([\"Device_ID\",\"time\"]).count()\n",
    "\n",
    "    # activity_per_bin.rename(columns={\"Device_ID\":\"bin_activity_fraction\"},inplace=True)\n",
    "    # activity_per_3h.rename(\"Activity\",inplace=True)\n",
    "    return activity_per_3h#.round().astype(int)\n",
    "\n",
    "activity_per_time_range = get_activity_per_time_bin(db_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_per_time_range[\"activity_fraction\"] = activity_per_time_range.groupby(\"Device_ID\").apply(lambda x: x/x.sum()).values\n",
    "activity_per_time_range = activity_per_time_range[[\"activity_fraction\"]].reset_index()\n",
    "activity_per_time_range = activity_per_time_range.pivot(index=\"Device_ID\",columns=\"time\",values=\"activity_fraction\")\n",
    "activity_per_time_range.columns = [f\"time_{col}\" for col in activity_per_time_range.columns]\n",
    "activity_per_time_range = (activity_per_time_range-activity_per_time_range.stack().min())/(activity_per_time_range.stack().max()-activity_per_time_range.stack().min())*2-1\n",
    "activity_per_time_range = activity_per_time_range.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot=(final_scores_pivot-final_scores_pivot.values.min())/(final_scores_pivot.values.max()-final_scores_pivot.values.min())*2-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = db_df.groupby(\"Device_ID\")[\"Target\"].first().reset_index().set_index(\"Device_ID\").join(final_scores_pivot)\n",
    "# final_features = final_features.join(daily_active_days)\n",
    "# final_features = final_features.join(activity_per_time_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create function to filter features based on mean values\n",
    "def filter_low_mean_features(features_df, percentile=0.1):\n",
    "    # Calculate absolute mean values for each feature\n",
    "    abs_means = abs(features_df).mean()\n",
    "    \n",
    "    # Calculate percentile threshold of absolute means\n",
    "    threshold = abs_means.quantile(percentile)\n",
    "    \n",
    "    # Get features with absolute means above threshold\n",
    "    significant_features = abs_means[abs_means >= threshold].index\n",
    "    \n",
    "    # Filter features\n",
    "\n",
    "    return significant_features\n",
    "\n",
    "# Apply the filtering function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.columns = [str(col) for col in final_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Split features and target\n",
    "# X_train = final_features[final_features.index.isin(train_devices)].drop('Target', axis=1)\n",
    "# significant_features = filter_low_mean_features(X_train, percentile=0.0)\n",
    "# X_train = X_train[significant_features]\n",
    "# y_train = final_features[final_features.index.isin(train_devices)]['Target']\n",
    "\n",
    "# # Get feature importances\n",
    "\n",
    "\n",
    "\n",
    "# X_test = final_features.loc[final_features.index.isin(test_device_ids),significant_features]\n",
    "# y_test = final_features[final_features.index.isin(test_device_ids)]['Target']\n",
    "\n",
    "# # Train Random Forest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate ROC AUC score\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# print(f'ROC AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train = final_features[final_features.index.isin(train_devices)].drop('Target', axis=1)\n",
    "y_train = final_features[final_features.index.isin(train_devices)]['Target']\n",
    "\n",
    "# Get feature importances\n",
    "\n",
    "\n",
    "\n",
    "X_test = final_features.loc[final_features.index.isin(test_device_ids)]\n",
    "y_test = final_features[final_features.index.isin(test_device_ids)]['Target']\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(random_state=0, subsample=0.8, colsample_bytree=0.8, learning_rate= 0.1,\n",
    "                               n_estimators= 150, max_depth=6, objective ='binary:logistic' ,eval_metric =roc_auc_score)\n",
    "selector = RFE(xgb_reg, n_features_to_select=1000, step=20000)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "best_features = list(X_train.columns[selector.support_])\n",
    "test_prediction = selector.estimator_.predict(X_test[best_features])\n",
    "print(f'The auc for validation set: {round(roc_auc_score(y_test,test_prediction), 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mafat-challenge-UwfjGg_R-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
