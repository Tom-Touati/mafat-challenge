{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The size of /dev/shm is too small (5885005824 bytes). The required size at least half of RAM (185619038208 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
      "2025-03-10 18:13:57,839\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 2075 MiB, 68 objects, write throughput 436 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 5156 MiB, 167 objects, write throughput 760 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Device_ID                   Datetime      URL  Domain_Name  Domain_cls1  \\\n",
      "0        124  2023-04-23 03:04:30+03:00     6466      2368671          755   \n",
      "1        124  2023-04-23 03:04:30+03:00  2245864      1792903            0   \n",
      "2        124  2023-04-23 03:04:30+03:00  1839478       107342          332   \n",
      "3        124  2023-04-23 03:14:50+03:00  1172090       107342          332   \n",
      "4        124  2023-04-23 03:14:50+03:00  1839478       107342          332   \n",
      "\n",
      "   Domain_cls2  Domain_cls3  Domain_cls4  Target  \n",
      "0          799            0            0       0  \n",
      "1            0            0            0       0  \n",
      "2            0            0            0       0  \n",
      "3            0            0            0       0  \n",
      "4            0            0            0       0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        df = mpd.read_sql(\"SELECT * FROM data\", con)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "dbfile = '/workspace/data/mini_training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 8205 MiB, 313 objects, write throughput 525 MiB/s.\n",
      "\u001b[36m(remote_exec_func pid=199698)\u001b[0m UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /workspace/.miniconda3/lib/python3.10/site-packages/libpyg.so)\n",
      "\u001b[36m(remote_exec_func pid=199698)\u001b[0m UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /workspace/.miniconda3/lib/python3.10/site-packages/libpyg.so)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          2023-04-23 03:04:30+03:00\n",
       "1          2023-04-23 03:04:30+03:00\n",
       "2          2023-04-23 03:04:30+03:00\n",
       "3          2023-04-23 03:14:50+03:00\n",
       "4          2023-04-23 03:14:50+03:00\n",
       "                      ...           \n",
       "32248978   2023-05-13 21:26:02+03:00\n",
       "32248979   2023-05-13 21:26:03+03:00\n",
       "32248980   2023-05-13 21:32:33+03:00\n",
       "32248981   2023-05-13 21:32:38+03:00\n",
       "32248982   2023-05-13 21:32:43+03:00\n",
       "Name: Datetime, Length: 32237806, dtype: datetime64[ns, UTC+03:00]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"23-04 to 18-05\"\n",
    "db_df = db_df[db_df[\"Domain_Name\"]!=1732927] # remove empty url\n",
    "db_df[\"Datetime\"] = mpd.to_datetime(db_df[\"Datetime\"])\n",
    "db_df[\"Datetime\"]\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x-x[\"Datetime\"].min()).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile preprocessing.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_masks(domain_counts, test_size=0.2, random_state=42):\n",
    "    # Get unique device IDs and their corresponding targets\n",
    "    device_target_df = domain_counts.groupby('Device_ID')['Target'].first().reset_index()\n",
    "    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    \n",
    "    # Create mask for train/test split in domain_counts\n",
    "    train_mask = domain_counts['Device_ID'].isin(train_device_ids)\n",
    "    test_mask = domain_counts['Device_ID'].isin(test_device_ids)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total devices: {len(device_target_df)}\")\n",
    "    print(f\"Train devices: {len(train_device_ids)}\")\n",
    "    print(f\"Test devices: {len(test_device_ids)}\")\n",
    "    print(f\"\\nTrain samples: {len(domain_counts[train_mask])}\")\n",
    "    print(f\"Test samples: {len(domain_counts[test_mask])}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nTarget distribution in train set:\")\n",
    "    print(domain_counts[train_mask].groupby('Target').size() / len(domain_counts[train_mask]))\n",
    "    print(\"\\nTarget distribution in test set:\")\n",
    "    print(domain_counts[test_mask].groupby('Target').size() / len(domain_counts[test_mask]))\n",
    "    \n",
    "    return train_mask, test_mask\n",
    "\n",
    "def get_domain_counts(db_df, pivot=False):\n",
    "    domain_counts = db_df.groupby([\"Device_ID\",\"Domain_Name\",\"Target\"]).count()\n",
    "    domain_counts = domain_counts.reset_index()\n",
    "    domain_counts = domain_counts[[\"Device_ID\",\"Domain_Name\",\"Target\",\"Datetime\"]]\n",
    "    domain_counts.rename(columns={\"Datetime\":\"count\"}, inplace=True)\n",
    "    if pivot:\n",
    "        pivot_matrix = domain_counts.pivot(index='source', columns='target', values='count').fillna(0)\n",
    "        return pivot_matrix\n",
    "    return domain_counts\n",
    "def get_device_domain_fractions(domain_counts,inplace=True):\n",
    "    # Calculate total counts per device\n",
    "    device_totals = domain_counts.groupby('Device_ID')['count'].sum()\n",
    "    \n",
    "    # Calculate fractions by dividing each count by the device total\n",
    "    domain_fractions = domain_counts if inplace else domain_counts.copy()\n",
    "    domain_fractions['fraction'] = domain_fractions.apply(\n",
    "        lambda row: row['count'] / device_totals[row['Device_ID']], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return domain_fractions\n",
    "def compute_domain_target_correlation(domain_counts):\n",
    "    # Group by Domain_Name and calculate mean Target and count\n",
    "    domain_stats = domain_counts.groupby('Domain_Name').agg({\n",
    "        'Target': 'mean',\n",
    "        'count': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    domain_stats.columns = ['Domain_Name', 'target_mean', 'count_mean', 'count_std', 'n_devices']\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    # We only include domains that appear in multiple devices for statistical significance\n",
    "    significant_domains = domain_stats[domain_stats['n_devices'] > 1]\n",
    "    \n",
    "    # Calculate correlation and p-value\n",
    "    correlation = mpd.DataFrame({\n",
    "        'Domain_Name': significant_domains['Domain_Name'],\n",
    "        'target_correlation': significant_domains['target_mean'],\n",
    "        'avg_count': significant_domains['count_mean'],\n",
    "        'count_std': significant_domains['count_std'],\n",
    "        'n_devices': significant_domains['n_devices']\n",
    "    }).sort_values('target_correlation', ascending=False)\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep in mind that the best resolution will be achieved with a resolution of urls/chain of urls, not domains\n",
      "Cluster url walks\n",
      "I want to cluster urls/url walks from a given domain, to 3 categories\n",
      "positive correlation, zero correlation, negative correlation, to_label\n"
     ]
    }
   ],
   "source": [
    "print(\"keep in mind that the best resolution will be achieved with a resolution of urls/chain of urls, not domains\")\n",
    "print(\"Cluster url walks\")\n",
    "print(\"I want to cluster urls/url walks from a given domain, to 3 categories\")\n",
    "print(\"positive correlation, zero correlation, negative correlation, to_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Basic features ( Domain Counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total devices: 615\n",
      "Train devices: 492\n",
      "Test devices: 123\n",
      "\n",
      "Train samples: 229983\n",
      "Test samples: 57590\n",
      "\n",
      "Target distribution in train set:\n",
      "Target\n",
      "0    0.466521\n",
      "1    0.533479\n",
      "dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "Target\n",
      "0    0.437003\n",
      "1    0.562997\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "domain_counts = get_domain_counts(db_df)\n",
    "del db_df\n",
    "device_domain_fractions = get_device_domain_fractions(domain_counts)\n",
    "device_domain_fractions[\"min_max_fraction\"] = (device_domain_fractions[\"fraction\"]-device_domain_fractions[\"fraction\"].min())/(device_domain_fractions[\"fraction\"].max()-device_domain_fractions[\"fraction\"].min())\n",
    "domain_target_correlation = compute_domain_target_correlation(domain_counts)\n",
    "\n",
    "train_mask, test_mask = get_train_test_masks(domain_counts)\n",
    "train_devices = domain_counts.loc[train_mask,\"Device_ID\"]\n",
    "test_devices = domain_counts.loc[test_mask,\"Device_ID\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction_pivot_matrix = device_domain_fractions.pivot(index='Domain_Name', columns='Device_ID', values='fraction').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile correlation.py\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "def calculate_chunk_correlation(matrix_centered, std_values, start_idx, end_idx, columns):\n",
    "    # Calculate correlation for this chunk\n",
    "    chunk = matrix_centered[:, start_idx:end_idx]\n",
    "    # Avoid division by zero by handling zero standard deviations\n",
    "    valid_std = (std_values > 0)\n",
    "    chunk_corr = np.zeros((end_idx - start_idx, len(columns)))\n",
    "    \n",
    "    # Calculate correlation only for columns with non-zero standard deviation\n",
    "    if valid_std.any():\n",
    "        chunk_corr = np.dot(chunk.T, matrix_centered[:, valid_std])\n",
    "        chunk_corr /= ((matrix_centered.shape[0] - 1) * \n",
    "                      np.outer(std_values[start_idx:end_idx], \n",
    "                             std_values[valid_std]))\n",
    "    \n",
    "    return start_idx, end_idx, chunk_corr\n",
    "@ray.remote\n",
    "def calculate_chunk_covariance(matrix_centered, start_idx, end_idx, columns):\n",
    "    # Calculate covariance for this chunk\n",
    "    chunk = matrix_centered[:, start_idx:end_idx]\n",
    "    chunk_cov = np.dot(chunk.T, matrix_centered) / (matrix_centered.shape[0] - 1)\n",
    "    \n",
    "    return start_idx, end_idx, chunk_cov\n",
    "\n",
    "def compute_chunked_metric(pivot_matrix, batch_size=2000,metric=\"covariance\"):\n",
    "    # Convert to numpy array for faster computation\n",
    "    matrix_dense = pivot_matrix.to_numpy()\n",
    "    matrix_centered = matrix_dense - np.mean(matrix_dense, axis=0)\n",
    "\n",
    "    # Initialize parameters\n",
    "    n_cols = pivot_matrix.shape[1]\n",
    "    futures = []\n",
    "    metric_func = calculate_chunk_covariance if metric==\"covariance\" else calculate_chunk_correlation if metric==\"correlation\" else None\n",
    "    # Submit tasks to Ray\n",
    "    for i in range(0, n_cols, batch_size):\n",
    "        batch_end = min(i + batch_size, n_cols)\n",
    "        futures.append(metric_func.remote(matrix_centered, i, batch_end, pivot_matrix.columns))\n",
    "\n",
    "    # Collect results and combine\n",
    "    cov_chunks = []\n",
    "    for future in ray.get(futures):\n",
    "        start_idx, end_idx, chunk_cov = future\n",
    "        chunk_df = mpd.DataFrame(\n",
    "            chunk_cov,\n",
    "            index=pivot_matrix.columns[start_idx:end_idx],\n",
    "            columns=pivot_matrix.columns\n",
    "        )\n",
    "        cov_chunks.append(chunk_df)\n",
    "\n",
    "    # Combine all chunks\n",
    "    return mpd.concat(cov_chunks)\n",
    "def melt_covariance_matrix(covariance_matrix):\n",
    "    # Reset index to make it a column\n",
    "    melted = covariance_matrix.reset_index()\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    melted = melted.melt(\n",
    "        id_vars=['Device_ID'],\n",
    "        var_name='target',\n",
    "        value_name='covariance'\n",
    "    )\n",
    "    \n",
    "    # Rename the 'index' column to 'source'\n",
    "    melted = melted.rename(columns={'Device_ID': 'source'})\n",
    "    \n",
    "    # Remove duplicate pairs (e.g., if A->B exists, remove B->A)\n",
    "    melted = melted[melted['source'] < melted['target']]\n",
    "    \n",
    "    # Remove rows where source equals target\n",
    "    melted = melted[melted['source'] != melted['target']]\n",
    "    \n",
    "    return melted.reset_index(drop=True)\n",
    "def filter_and_transform_covariance(melted_covariance, abs_threshold=0.03, log_threshold=10):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result = melted_covariance\n",
    "    # Create corrected_cov column\n",
    "    result['corrected_cov'] = result['covariance'].copy()\n",
    "    \n",
    "    # Apply absolute threshold filter\n",
    "    result.loc[abs(result['corrected_cov']) < abs_threshold, 'corrected_cov'] = 0\n",
    "    \n",
    "    # Apply log transformation for values above log_threshold\n",
    "    high_vals_mask = abs(result['corrected_cov']) > log_threshold\n",
    "    result.loc[high_vals_mask, 'corrected_cov'] = result.loc[high_vals_mask, 'corrected_cov'].apply(\n",
    "        lambda x: np.log(abs(x)) * np.sign(x)\n",
    "    )\n",
    "    print(high_vals_mask.mean())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Covariance/Correlation between domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "UserWarning: `melt` implementation has mismatches with pandas:\n",
      "Order of rows could be different from pandas.\n"
     ]
    }
   ],
   "source": [
    "# Get domain counts and create pivot matrix\n",
    "\n",
    "# Compute covariance matrix and melt it\n",
    "covariance_matrix = compute_chunked_metric(train_fraction_pivot_matrix) * 10000\n",
    "melted_cov = melt_covariance_matrix(covariance_matrix)\n",
    "melted_cov[\"min_max_cov\"] = (melted_cov[\"covariance\"]-melted_cov[\"covariance\"].min())/(melted_cov[\"covariance\"].max()-melted_cov[\"covariance\"].min())\n",
    "\n",
    "# corrected_cov = filter_and_transform_covariance(melted_cov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile graph_model_funcs.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_NODE2_VEC_PARAMS = dict(embedding_dim=128,\n",
    "walk_length=80,\n",
    "context_size=10,\n",
    "walks_per_node=10,\n",
    "p=1,\n",
    "q=1.5,\n",
    "sparse=True)\n",
    "def create_graph_data(corrected_cov,weight_col):\n",
    "    # Convert domain names to numerical indices\n",
    "    unique_domains = list(set(corrected_cov['source']).union(corrected_cov['target']))\n",
    "    domain_to_idx = {domain: idx for idx, domain in enumerate(unique_domains)}\n",
    "    \n",
    "    # Create edge index and edge weights\n",
    "    edge_index = torch.tensor([\n",
    "        [domain_to_idx[s] for s in corrected_cov['source']],\n",
    "        [domain_to_idx[t] for t in corrected_cov['target']]\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    edge_weight = torch.tensor(corrected_cov[weight_col].values, dtype=torch.float)\n",
    "    \n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weight,\n",
    "        num_nodes=len(unique_domains)\n",
    "    )\n",
    "    return data, domain_to_idx\n",
    "\n",
    "def train_node2vec(data, node2vec_params=DEFAULT_NODE2_VEC_PARAMS,device='cuda'):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = Node2Vec(\n",
    "        data.edge_index,\n",
    "        **node2vec_params\n",
    "    ).to(device)\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "            \n",
    "    return model\n",
    "@ray.remote\n",
    "def _compute_device_embedding(device_id, group, embeddings, domain_mapping):\n",
    "    valid_embeddings = []\n",
    "    valid_weights = []\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        domain = row['Domain_Name']\n",
    "        if domain in domain_mapping:\n",
    "            idx = domain_mapping[domain]\n",
    "            valid_embeddings.append(embeddings[idx])\n",
    "            valid_weights.append(row['fraction'])\n",
    "    \n",
    "    if valid_embeddings:\n",
    "        valid_weights = np.array(valid_weights)\n",
    "        valid_weights = valid_weights / valid_weights.sum()\n",
    "        device_embedding = np.average(valid_embeddings, weights=valid_weights, axis=0)\n",
    "        return device_id, device_embedding\n",
    "    return device_id, None\n",
    "\n",
    "def compute_device_embeddings(device_fractions, embeddings, domain_mapping):\n",
    "    # Group device fractions by Device_ID\n",
    "    grouped_fractions = device_fractions.groupby('Device_ID')\n",
    "    \n",
    "    # Create remote tasks\n",
    "    futures = [\n",
    "        _compute_device_embedding.remote(device_id, group, embeddings, domain_mapping)\n",
    "        for device_id, group in grouped_fractions\n",
    "    ]\n",
    "    \n",
    "    # Collect results\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    # Convert results to dictionary\n",
    "    device_embeddings = {\n",
    "        device_id: embedding \n",
    "        for device_id, embedding in results \n",
    "        if embedding is not None\n",
    "    }\n",
    "    \n",
    "    return device_embeddings\n",
    "# def create_graph_and_get_embedding(corrected_cov,weight_col,device='cuda'node2vec_params):\n",
    "#     data, domain_mapping = create_graph_data(corrected_cov,weight_col)\n",
    "#     model = train_node2vec(data,device=device,**node2vec_params)\n",
    "#     embeddings = model.embedding.weight.cpu().detach().numpy()\n",
    "#     # device_embeddings = compute_device_embeddings(device_domain_fractions, embeddings, domain_mapping)\n",
    "#     return embeddings,domain_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mask, test_mask = get_train_test_masks(domain_counts)\n",
    "final_device_domain_fractions = device_domain_fractions[[\"Device_ID\",\"Domain_Name\",\"min_max_fraction\"]].rename(columns={\"Domain_Name\":\"target\",\"Device_ID\":\"source\",\"min_max_fraction\":\"weight\"})\n",
    "final_cov = melted_cov[[\"source\",\"target\",\"min_max_cov\"]].rename(columns={\"min_max_cov\":\"weight\"})\n",
    "final_df = mpd.concat([final_device_domain_fractions,final_cov],ignore_index=True)\n",
    "data, domain_mapping = create_graph_data(final_df,weight_col=\"weight\")\n",
    "\n",
    "model = train_node2vec(data)\n",
    "embeddings = model().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict(embedding_dim=128,\n",
    "walk_length=80,\n",
    "context_size=10,\n",
    "walks_per_node=10,\n",
    "p=1,\n",
    "q=1.5,\n",
    "sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_xgboost(embeddings, node_map_df, domain_counts, train_devices, test_devices):\n",
    "    # Get device embeddings for training and testing\n",
    "\n",
    "    train_emb_indices = node_map_df.loc[train_devices]\n",
    "    test_emb_indices = node_map_df.loc[test_devices]\n",
    "    train_device_embeddings = mpd.DataFrame(embeddings[train_emb_indices],index=train_devices)\n",
    "    test_device_embeddings = mpd.DataFrame(embeddings[test_emb_indices],index=test_devices)\n",
    "    domain_counts_w_index = domain_counts.drop_duplicates(subset=\"Device_ID\").set_index(\"Device_ID\")\n",
    "\n",
    "    # Prepare training data\n",
    "    X_train = train_device_embeddings\n",
    "    y_train = domain_counts_w_index.loc[train_devices, 'Target'].values\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = test_device_embeddings\n",
    "    y_test = domain_counts_w_index.loc[test_devices, 'Target'].values\n",
    "\n",
    "    # Initialize and train Logistic Regression\n",
    "    # Initialize and train XGBoost classifier\n",
    "    xgb_model = xgb.XGBRegressor(objective='binary:logistic', random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions and calculate ROC AUC score\n",
    "    y_pred_proba = xgb_model.predict(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    return roc_auc\n",
    "def train_knn(embeddings, node_map_dff, domain_counts, train_devices, test_devices, n_neighbors):\n",
    "    # Get device embeddings for training and testing\n",
    "    print(\"here\")\n",
    "    print(type(train_devices))\n",
    "    print(type(test_devices))\n",
    "    print(type(node_map_dff))\n",
    "    train_emb_indices = node_map_dff.loc[train_devices,\"idx\"].values\n",
    "    print(train_devices)\n",
    "    test_emb_indices = node_map_dff.loc[test_devices,\"idx\"].values\n",
    "    print(test_devices)\n",
    "    print(\"out\")\n",
    "    train_device_embeddings = mpd.DataFrame(embeddings[train_emb_indices], index=train_devices)\n",
    "    test_device_embeddings = mpd.DataFrame(embeddings[test_emb_indices], index=test_devices)\n",
    "    domain_counts_w_index = domain_counts.drop_duplicates(subset=\"Device_ID\").set_index(\"Device_ID\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_device_embeddings\n",
    "    y_train = domain_counts_w_index.loc[train_devices, 'Target'].values\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_device_embeddings\n",
    "    y_test = domain_counts_w_index.loc[test_devices, 'Target'].values\n",
    "    \n",
    "    # Initialize and train KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions and calculate ROC AUC score\n",
    "    y_pred_proba = knn.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "node_to_vec_params = dict(embedding_dim=128,\n",
    "walk_length=80,\n",
    "context_size=10,\n",
    "walks_per_node=10,\n",
    "p=1,\n",
    "q=1.5,\n",
    "sparse=True)\n",
    "grid_dict = {\"embedding_dim\":[32,64,128,256],\"walk_length\":[20,40,80,160],\"context_size\":[5,10,20],\"walks_per_node\":[5,10,20],\"p\":[0.5,1,1.5],\"q\":[0.5,1,1.5],\"sparse\":[True]}\n",
    "# Get all combinations of parameters\n",
    "param_keys = list(grid_dict.keys())\n",
    "param_values = list(grid_dict.values())\n",
    "param_combinations = list(itertools.product(*param_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 19:59:38,171\tINFO worker.py:1672 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     param_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_keys, params))\n\u001b[1;32m     41\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(train_node2vec_and_evaluate\u001b[38;5;241m.\u001b[39mremote(data\u001b[38;5;241m=\u001b[39mdata_id,node_map_dff\u001b[38;5;241m=\u001b[39mnode_map_id,domain_counts\u001b[38;5;241m=\u001b[39m domain_counts_id,train_devices\u001b[38;5;241m=\u001b[39m train_devices_id, test_devices\u001b[38;5;241m=\u001b[39mtest_devices_id, params\u001b[38;5;241m=\u001b[39m param_dict))\n\u001b[0;32m---> 42\u001b[0m results \u001b[38;5;241m=\u001b[39m[ray\u001b[38;5;241m.\u001b[39mget(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Collect results\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# for i, future in enumerate(ray.get(futures)):\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     params = dict(zip(param_keys, param_combinations[i]))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(\"\\nBest results:\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(results_df.sort_values('auc', ascending=False).head())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     param_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_keys, params))\n\u001b[1;32m     41\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(train_node2vec_and_evaluate\u001b[38;5;241m.\u001b[39mremote(data\u001b[38;5;241m=\u001b[39mdata_id,node_map_dff\u001b[38;5;241m=\u001b[39mnode_map_id,domain_counts\u001b[38;5;241m=\u001b[39m domain_counts_id,train_devices\u001b[38;5;241m=\u001b[39m train_devices_id, test_devices\u001b[38;5;241m=\u001b[39mtest_devices_id, params\u001b[38;5;241m=\u001b[39m param_dict))\n\u001b[0;32m---> 42\u001b[0m results \u001b[38;5;241m=\u001b[39m[\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Collect results\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# for i, future in enumerate(ray.get(futures)):\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     params = dict(zip(param_keys, param_combinations[i]))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(\"\\nBest results:\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(results_df.sort_values('auc', ascending=False).head())\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.10/site-packages/ray/_private/worker.py:2771\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2766\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2767\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2768\u001b[0m     )\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2771\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.10/site-packages/ray/_private/worker.py:893\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    884\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    886\u001b[0m         )\n\u001b[1;32m    888\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    891\u001b[0m data_metadata_pairs: List[\n\u001b[1;32m    892\u001b[0m     Tuple[ray\u001b[38;5;241m.\u001b[39m_raylet\u001b[38;5;241m.\u001b[39mBuffer, \u001b[38;5;28mbytes\u001b[39m]\n\u001b[0;32m--> 893\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3189\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/includes/common.pxi:83\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "@ray.remote\n",
    "def train_node2vec_and_evaluate(data,node_map_dff, domain_counts, train_devices, test_devices, params):\n",
    "    # Print parameters\n",
    "    print(\"Training with parameters:\")\n",
    "    for k,v in params.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = train_node2vec(data, params)\n",
    "    embeddings = model().detach().cpu().numpy()\n",
    "    \n",
    "    # Evaluate with different k values\n",
    "    k_values = [4, 8, 12, 16]\n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        auc = train_knn(embeddings=embeddings,node_map_dff= node_map_dff, domain_counts=domain_counts,train_devices= train_devices,test_devices= test_devices, n_neighbors=k)\n",
    "        results.append((k, auc))\n",
    "        \n",
    "    print(\"\\nKNN Results:\")\n",
    "    \n",
    "    print(\"params:\\n\",params,\"\\n\",\"res:\\n\",results)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "# train_node2vec_and_evaluate(data, domain_counts, train_devices, test_devices, node_to_vec_params)\n",
    "# Create list to store results\n",
    "all_results = []\n",
    "node_map_df = mpd.Series(domain_mapping,name=\"idx\").to_frame()\n",
    "# Submit tasks for each parameter combination\n",
    "futures = []\n",
    "data_id = ray.put(data)\n",
    "node_map_id = ray.put(node_map_df)\n",
    "domain_counts_id = ray.put(domain_counts)\n",
    "train_devices_id = ray.put(train_devices.values)\n",
    "test_devices_id = ray.put(test_devices.values)\n",
    "for params in param_combinations:\n",
    "    param_dict = dict(zip(param_keys, params))\n",
    "    futures.append(train_node2vec_and_evaluate.remote(data=data_id,node_map_dff=node_map_id,domain_counts= domain_counts_id,train_devices= train_devices_id, test_devices=test_devices_id, params= param_dict))\n",
    "results =[ray.get(f) for f in futures]\n",
    "\n",
    "# Collect results\n",
    "# for i, future in enumerate(ray.get(futures)):\n",
    "#     params = dict(zip(param_keys, param_combinations[i]))\n",
    "#     for k, auc in future:\n",
    "#         result = {**params, 'k': k, 'auc': auc}\n",
    "#         all_results.append(result)\n",
    "\n",
    "# # Convert results to dataframe for analysis\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "# print(\"\\nBest results:\")\n",
    "# print(results_df.sort_values('auc', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= [ray.get(f) for f in futures[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 5, 'p': 0.5, 'q': 0.5, 'sparse': True} \n",
      " score:\n",
      " 0.4645257218406281\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 5, 'p': 1, 'q': 1, 'sparse': True} \n",
      " score:\n",
      " 0.4817967444025432\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 5, 'p': 1, 'q': 1.5, 'sparse': True} \n",
      " score:\n",
      " 0.49379449046216506\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 5, 'p': 1.5, 'q': 0.5, 'sparse': True} \n",
      " score:\n",
      " 0.528438748893382\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 10, 'p': 0.5, 'q': 1, 'sparse': True} \n",
      " score:\n",
      " 0.5672515602437653\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 20, 'p': 0.5, 'q': 0.5, 'sparse': True} \n",
      " score:\n",
      " 0.5748448472031522\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 5, 'walks_per_node': 20, 'p': 1.5, 'q': 1.5, 'sparse': True} \n",
      " score:\n",
      " 0.5985685619751635\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 20, 'context_size': 10, 'walks_per_node': 20, 'p': 0.5, 'q': 0.5, 'sparse': True} \n",
      " score:\n",
      " 0.6024376932010587\n",
      "params:\n",
      " {'embedding_dim': 32, 'walk_length': 40, 'context_size': 5, 'walks_per_node': 5, 'p': 0.5, 'q': 1.5, 'sparse': True} \n",
      " score:\n",
      " 0.6620816243916019\n"
     ]
    }
   ],
   "source": [
    "max_score = 0.0\n",
    "for params,results in zip(param_combinations,futures):\n",
    "    res=ray.get(results)\n",
    "    score = res[0][1]\n",
    "    param_dict = dict(zip(param_keys, params))\n",
    "    if  score>= max_score:\n",
    "        max_score = score\n",
    "        print(\"params:\\n\",param_dict,\"\\n\",\"score:\\n\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.4410\n",
      "ROC AUC Score: 0.5728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5727730084014633"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_xgboost(embeddings, node_map_df, domain_counts, train_devices, test_devices)\n",
    "train_knn(embeddings, node_map_df, domain_counts, train_devices, test_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "import umap\n",
    "\n",
    "\n",
    "# Initialize Init\n",
    "reducer umap.UMAP(random_state= =42)\n",
    "\n",
    "# Get device embeddings for visualization\n",
    "train_emb_indices = node_map_df.loc[train_devices]\n",
    "test_emb_indices = node_map_df.loc[test_devices]\n",
    "\n",
    "# Combine train and test embeddings\n",
    "all_embeddings = embeddings[np.concatenate([train_emb_indices, test_emb_indices])]\n",
    "labels = np.concatenate([domain_counts[train_mask]['Target'].values, \n",
    "                        domain_counts[test_mask]['Target'].values])\n",
    "\n",
    "\n",
    "# Fit and transform the data\n",
    "embedding_2d = reducer.fit_transform(all_embeddings) umap.UMAP(random_state\n",
    "\n",
    "test_emb_indices = node_map_df.loc[test_devices]\n",
    "\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "plt.title('UMAP visualization of embeddings')\n",
    "plt.show()\n",
    "# Create scatter plot\n",
    "                     c=labels, cmap='viridis', alpha=0.6)\n",
    "scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "plt.figure(figsize=(10, 8))embedding_2d = reducer.fit_transform(all_embeddings)\n",
    "all_embeddings = embeddings[np.concatenate([train_emb_indices, test\n",
    "_e\n",
    "                        domain_counts[test_mask]['Target'].values])mb_indices])]\n",
    "labels = np.concatenate([domain_counts[train_mask]['Target'].values, \n",
    "# Combine train and test embeddings\n",
    "#= Get device embeddings42) for visualiza\n",
    "train_emb_indices = node_map_df.loc[train_devices]tionialize UMAP UMAP\n",
    "\n",
    "import\n",
    "plt.show()\n",
    "plt.title('UMAP visualization of embeddings')\n",
    "plt.colorbar(scatter)\n",
    "                     c=labels, cmap='viridis', alpha=0.6)\n",
    "scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "plt.figure(figsize=(10, 8))\n",
    "# Create scatter plot\n",
    "\n",
    "embedding_2d = reducer.fit_transform(all_embeddings)\n",
    "# Fit and transform the data\n",
    "\n",
    "                        domain_counts[test_mask]['Target'].values])\n",
    "labels = np.concatenate([domain_counts[train_mask]['Target'].values, \n",
    "all_embeddings = embeddings[np.concatenate([train_emb_indices, test_emb_indices])]\n",
    "# Combine train and test embeddings\n",
    "\n",
    "test_emb_indices = node_map_df.loc[test_devices]\n",
    "train_emb_indices = node_map_df.loc[train_devices]\n",
    "# Get device embeddings for visualization\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "# Initialize UMAP\n",
    " matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as matplotlib.pyplot plt as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
