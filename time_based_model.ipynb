{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fresh Idea\n",
    "## separate one/zero activity of domains\n",
    "- replace zeros by minus one\n",
    "- calculate the class activity for 3 hours bins for each domain\n",
    "- calculate the user activity for gaussian around center of 3 hour bins\n",
    "- calculate the likelihood of the person being a 1/-1 in that time \n",
    "- add user general metrics including domain cls activity and usage patterns\n",
    "\n",
    "### questions\n",
    "- how to take into account times when the person used a website when others didnt?\n",
    "- how to give likelihood when the person didn't show any nearby activity?\n",
    "- what if he used a similar website at same time but more nich? \n",
    "- how to average the bins weighted by the significance of that bin?\n",
    "- how to give weight to the magnitude of number of users entering? probability of 1 with confidence\n",
    "- what about sparse websites?\n",
    "- how to not let times where there are no activity take a lot of weight?\n",
    "### enhancements\n",
    "- create graph embedding of urls\n",
    "- for each bin, calculate the metric per url\n",
    "- instead of only looking at the specific website, take into account websites with similar usages,\n",
    "  for example looking at same domain_cls usage in gaussian around bin, or looking at domain embeddings and looking at the activity in similar embeddings weighted by the distance in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 19:42:11,663\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Device_ID                   Datetime      URL  Domain_Name  Domain_cls1  \\\n",
      "0        124  2023-04-23 03:04:30+03:00     6466      2368671          755   \n",
      "1        124  2023-04-23 03:04:30+03:00  2245864      1792903            0   \n",
      "2        124  2023-04-23 03:04:30+03:00  1839478       107342          332   \n",
      "3        124  2023-04-23 03:14:50+03:00  1172090       107342          332   \n",
      "4        124  2023-04-23 03:14:50+03:00  1839478       107342          332   \n",
      "\n",
      "   Domain_cls2  Domain_cls3  Domain_cls4  Target  \n",
      "0          799            0            0       0  \n",
      "1            0            0            0       0  \n",
      "2            0            0            0       0  \n",
      "3            0            0            0       0  \n",
      "4            0            0            0       0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "import ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        df = mpd.read_sql(\"SELECT * FROM data\", con)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "dbfile = '/workspace/data/mini_training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"23-04 to 18-05\"\n",
    "db_df = db_df[db_df[\"Domain_Name\"]!=1732927] # remove empty url\n",
    "db_df[\"Datetime\"] = mpd.to_datetime(db_df[\"Datetime\"])\n",
    "db_df.set_index(\"Datetime\",inplace=True)\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x-x[\"Datetime\"].min()).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_devices(device_target_df, test_size=0.2, random_state=42):    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    return train_device_ids, test_device_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = db_df.groupby(\"Device_ID\").first().reset_index()\n",
    "train_devices, test_device_ids = get_train_test_devices(devices)\n",
    "train_db_df = db_df[db_df[\"Device_ID\"].isin(train_devices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_per_domain = train_db_df.groupby(\"Domain_Name\")[\"Device_ID\"].nunique()\n",
    "domains = devices_per_domain[devices_per_domain>10].index\n",
    "train_db_df = train_db_df[train_db_df[\"Domain_Name\"].isin(domains)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess timeseries\n",
    "def process_activity_timeseries(domain_df,bin_hours=6,gaussian_filter=True,n_days_each_side=3,std=1.5,drop_na=True,drop_zeros=False):\n",
    "    activity_per_3h = domain_df[\"Device_ID\"].resample(f'{str(bin_hours)}h').nunique()\n",
    "    gaussian_window_hours = int(n_days_each_side*24/bin_hours*2) # n_days_each_side * 24h / 3h_per_bin * 2 sides\n",
    "    if gaussian_filter:\n",
    "        activity_per_3h = activity_per_3h.rolling(window=gaussian_window_hours, win_type='gaussian',center=True,min_periods=1,closed=\"both\").mean(std=std)\n",
    "    if drop_na:\n",
    "        activity_per_3h.dropna(inplace=True)\n",
    "    if drop_zeros:\n",
    "        activity_per_3h = activity_per_3h[activity_per_3h!=0]\n",
    "    activity_per_3h.rename(\"Activity\",inplace=True)\n",
    "    return activity_per_3h.round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_deploy_ray_func pid=1195589)\u001b[0m FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "\u001b[36m(_deploy_ray_func pid=1195606)\u001b[0m FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "\u001b[36m(remote_exec_func pid=1195602)\u001b[0m FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "\u001b[36m(remote_exec_func pid=1195602)\u001b[0m FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "\u001b[36m(remote_exec_func pid=1195602)\u001b[0m FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "\u001b[36m(remote_exec_func pid=1195602)\u001b[0m FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "process_domain_timeseries = partial(process_activity_timeseries,gaussian_filter=True,n_days_each_side=3,std=1.5,drop_na=True,drop_zeros=False)\n",
    "process_domain_timeseries.__name__ =process_activity_timeseries.__name__\n",
    "domain_time_series = train_db_df.groupby([\"Domain_Name\",\"Target\"]).apply(process_domain_timeseries)\n",
    "# domain_no_target_time_series = train_db_df.groupby([\"Domain_Name\"]).apply(process_domain_timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_domain_time_series = db_df.groupby([\"Device_ID\",\"Domain_Name\"]).apply(process_activity_timeseries)\n",
    "\n",
    "# Reset index to get all columns as regular columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_domain_time_series = user_domain_time_series.swaplevel(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: <function GroupBy.<lambda>> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "Please refer to https://modin.readthedocs.io/en/stable/supported_apis/defaulting_to_pandas.html for explanation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate domain fractions and activity separately\n",
    "# First, calculate activity fraction\n",
    "domain_fraction_ts = domain_time_series.copy()\n",
    "domain_fraction_ts[\"activity_fraction\"] = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"]).transform(lambda x: x/x.sum())\n",
    "# Add the sum of activity as a new column\n",
    "domain_activity = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"])[[\"Activity\"]].sum()\n",
    "\n",
    "domain_activity = domain_activity.rename(columns={\"Activity\": \"target_domain_activity\"})\n",
    "# Merge the results\n",
    "domain_fraction_ts = domain_fraction_ts.merge(domain_activity, left_index=True, right_index=True)\n",
    "# Reset index to get Target as a column, then pivot to get Target as columns\n",
    "pivot_fraction_ts = domain_fraction_ts.reset_index().pivot(\n",
    "    index=['Datetime', 'Domain_Name'],\n",
    "    columns='Target'\n",
    ").fillna(0)\n",
    "pivot_fraction_ts.columns = [f'{col[0]}_{col[1]}' for col in pivot_fraction_ts.columns]\n",
    "# Rename columns for clarity\n",
    "# pivot_fraction_ts.columns = ['activity_0', 'activity_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pivot_fraction_ts.merge(user_domain_time_series.reset_index(),how=\"left\",on=[\"Domain_Name\",\"Datetime\"],)\n",
    "merged_df.set_index([\"Datetime\",\"Domain_Name\",\"Device_ID\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def class_probability_score(active, p_active_given_a, p_active_given_b, prior_a=0.5, total_users=100):\n",
    "    \"\"\"\n",
    "    Calculate class probability score with vectorized operations\n",
    "    \n",
    "    Args:\n",
    "        active: Boolean indicating if user was active\n",
    "        p_active_given_a: Probability of activity given class A (0)\n",
    "        p_active_given_b: Probability of activity given class B (1)\n",
    "        prior_a: Prior probability for class A\n",
    "        total_users: Total number of users for confidence calculation\n",
    "    \"\"\"\n",
    "    # Use numpy for vectorized operations\n",
    "    likelihood_a = np.where(active, p_active_given_a, 1 - p_active_given_a)\n",
    "    likelihood_b = np.where(active, p_active_given_b, 1 - p_active_given_b)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    evidence = (likelihood_a * prior_a + likelihood_b * (1 - prior_a))\n",
    "    posterior_a = (likelihood_a * prior_a) / evidence\n",
    "    \n",
    "    # Simplified confidence calculation\n",
    "    # alpha = 1 + posterior_a * total_users\n",
    "    # beta = 1 + (1 - posterior_a) * total_users\n",
    "    # ci_width = stats.beta.interval(0.95, alpha, beta)[1] - stats.beta.interval(0.95, alpha, beta)[0]\n",
    "    \n",
    "    # # Calculate final score\n",
    "    raw_score = 2 * posterior_a - 1\n",
    "    # confidence_factor = 1 - ci_width\n",
    "    \n",
    "    return raw_score #* confidence_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"bin_activity\"] = merged_df[\"Activity_0\"]+merged_df[\"Activity_1\"]\n",
    "merged_df[\"total_activity\"] = (merged_df[\"target_domain_activity_0\"]+merged_df[\"target_domain_activity_1\"])\n",
    "merged_df[\"relative_0_activity\"] = merged_df[\"target_domain_activity_0\"]/merged_df[\"total_activity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: __array_ufunc__ is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: Distributing <class 'pandas.core.series.Series'> object. This may take some time.\n",
      "UserWarning: __array_ufunc__ is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: Distributing <class 'pandas.core.series.Series'> object. This may take some time.\n",
      "UserWarning: __array_ufunc__ is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: Distributing <class 'pandas.core.series.Series'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "merged_df[\"score\"]=class_probability_score(merged_df[\"Activity\"], merged_df[\"activity_fraction_0\"], merged_df[\"activity_fraction_1\"], prior_a=merged_df[\"relative_0_activity\"], total_users=merged_df[\"bin_activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: __array_ufunc__ is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: Distributing <class 'pandas.core.series.Series'> object. This may take some time.\n",
      "UserWarning: <function GroupBy.<lambda>> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    }
   ],
   "source": [
    "merged_df[\"weighted_score\"] = merged_df[\"score\"]*np.sqrt(merged_df[\"bin_activity\"])\n",
    "final_scores = merged_df.groupby([\"Device_ID\",\"Domain_Name\"])[\"weighted_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot = final_scores.to_frame().reset_index().pivot(index=\"Device_ID\",columns=\"Domain_Name\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = db_df.groupby(\"Device_ID\")[\"Target\"].first().reset_index().set_index(\"Device_ID\").join(final_scores_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create function to filter features based on mean values\n",
    "def filter_low_mean_features(features_df, percentile=0.1):\n",
    "    # Calculate absolute mean values for each feature\n",
    "    abs_means = abs(features_df).mean()\n",
    "    \n",
    "    # Calculate percentile threshold of absolute means\n",
    "    threshold = abs_means.quantile(percentile)\n",
    "    \n",
    "    # Get features with absolute means above threshold\n",
    "    significant_features = abs_means[abs_means >= threshold].index\n",
    "    \n",
    "    # Filter features\n",
    "\n",
    "    return significant_features\n",
    "\n",
    "# Apply the filtering function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.7071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Split features and target\n",
    "X_train = final_features[final_features.index.isin(train_devices)].drop('Target', axis=1)\n",
    "significant_features = filter_low_mean_features(X_train, percentile=0.1)\n",
    "X_train = X_train[significant_features]\n",
    "y_train = final_features[final_features.index.isin(train_devices)]['Target']\n",
    "\n",
    "# Get feature importances\n",
    "\n",
    "\n",
    "\n",
    "X_test = final_features.loc[final_features.index.isin(test_device_ids),significant_features]\n",
    "y_test = final_features[final_features.index.isin(test_device_ids)]['Target']\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
