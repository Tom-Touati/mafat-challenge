{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The size of /dev/shm is too small (12884901888 bytes). The required size at least half of RAM (185619038208 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
      "2025-03-09 00:33:53,439\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Device_ID                   Datetime      URL  Domain_Name  Domain_cls1  \\\n",
      "0        124  2023-04-23 03:04:30+03:00     6466      2368671          755   \n",
      "1        124  2023-04-23 03:04:30+03:00  2245864      1792903            0   \n",
      "2        124  2023-04-23 03:04:30+03:00  1839478       107342          332   \n",
      "3        124  2023-04-23 03:14:50+03:00  1172090       107342          332   \n",
      "4        124  2023-04-23 03:14:50+03:00  1839478       107342          332   \n",
      "\n",
      "   Domain_cls2  Domain_cls3  Domain_cls4  Target  \n",
      "0          799            0            0       0  \n",
      "1            0            0            0       0  \n",
      "2            0            0            0       0  \n",
      "3            0            0            0       0  \n",
      "4            0            0            0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 5659 MiB, 183 objects, write throughput 310 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        df = mpd.read_sql(\"SELECT * FROM data\", con)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "dbfile = '/workspace/code/train_data_for_competition/mini_training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2023-04-23 03:04:30+03:00\n",
       "1          2023-04-23 03:04:30+03:00\n",
       "2          2023-04-23 03:04:30+03:00\n",
       "3          2023-04-23 03:14:50+03:00\n",
       "4          2023-04-23 03:14:50+03:00\n",
       "                      ...           \n",
       "32248978   2023-05-13 21:26:02+03:00\n",
       "32248979   2023-05-13 21:26:03+03:00\n",
       "32248980   2023-05-13 21:32:33+03:00\n",
       "32248981   2023-05-13 21:32:38+03:00\n",
       "32248982   2023-05-13 21:32:43+03:00\n",
       "Name: Datetime, Length: 32248983, dtype: datetime64[ns, UTC+03:00]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"23-04 to 18-05\"\n",
    "db_df[\"Datetime\"] = mpd.to_datetime(db_df[\"Datetime\"])\n",
    "db_df[\"Datetime\"]\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x-x[\"Datetime\"].min()).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile preprocessing.py\n",
    "def get_train_test_masks(domain_counts, test_size=0.2, random_state=42):\n",
    "    # Get unique device IDs and their corresponding targets\n",
    "    device_target_df = domain_counts.groupby('Device_ID')['Target'].first().reset_index()\n",
    "    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    \n",
    "    # Create mask for train/test split in domain_counts\n",
    "    train_mask = domain_counts['Device_ID'].isin(train_device_ids)\n",
    "    test_mask = domain_counts['Device_ID'].isin(test_device_ids)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total devices: {len(device_target_df)}\")\n",
    "    print(f\"Train devices: {len(train_device_ids)}\")\n",
    "    print(f\"Test devices: {len(test_device_ids)}\")\n",
    "    print(f\"\\nTrain samples: {len(domain_counts[train_mask])}\")\n",
    "    print(f\"Test samples: {len(domain_counts[test_mask])}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nTarget distribution in train set:\")\n",
    "    print(domain_counts[train_mask].groupby('Target').size() / len(domain_counts[train_mask]))\n",
    "    print(\"\\nTarget distribution in test set:\")\n",
    "    print(domain_counts[test_mask].groupby('Target').size() / len(domain_counts[test_mask]))\n",
    "    \n",
    "    return train_mask, test_mask\n",
    "\n",
    "def get_domain_counts(db_df, pivot=False):\n",
    "    domain_counts = db_df.groupby([\"Device_ID\",\"Domain_Name\",\"Target\"]).count()\n",
    "    domain_counts = domain_counts.reset_index()\n",
    "    domain_counts = domain_counts[[\"Device_ID\",\"Domain_Name\",\"Target\",\"Datetime\"]]\n",
    "    domain_counts.rename(columns={\"Datetime\":\"count\"}, inplace=True)\n",
    "    if pivot:\n",
    "        pivot_matrix = train_domain_counts.pivot(index='source', columns='target', values='count').fillna(0)\n",
    "        return pivot_matrix\n",
    "    return domain_counts\n",
    "def get_device_domain_fractions(domain_counts):\n",
    "    # Calculate total counts per device\n",
    "    device_totals = domain_counts.groupby('Device_ID')['count'].sum()\n",
    "    \n",
    "    # Calculate fractions by dividing each count by the device total\n",
    "    domain_fractions = domain_counts.copy()\n",
    "    domain_fractions['fraction'] = domain_fractions.apply(\n",
    "        lambda row: row['count'] / device_totals[row['Device_ID']], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return domain_fractions[['Device_ID', 'Domain_Name', 'Target', 'count', 'fraction']]\n",
    "def compute_domain_target_correlation(domain_counts):\n",
    "    # Group by Domain_Name and calculate mean Target and count\n",
    "    domain_stats = domain_counts.groupby('Domain_Name').agg({\n",
    "        'Target': 'mean',\n",
    "        'count': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    domain_stats.columns = ['Domain_Name', 'target_mean', 'count_mean', 'count_std', 'n_devices']\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    # We only include domains that appear in multiple devices for statistical significance\n",
    "    significant_domains = domain_stats[domain_stats['n_devices'] > 1]\n",
    "    \n",
    "    # Calculate correlation and p-value\n",
    "    correlation = mpd.DataFrame({\n",
    "        'Domain_Name': significant_domains['Domain_Name'],\n",
    "        'target_correlation': significant_domains['target_mean'],\n",
    "        'avg_count': significant_domains['count_mean'],\n",
    "        'count_std': significant_domains['count_std'],\n",
    "        'n_devices': significant_domains['n_devices']\n",
    "    }).sort_values('target_correlation', ascending=False)\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"keep in mind that the best resolution will be achieved with a resolution of urls/chain of urls, not domains\")\n",
    "print(\"Cluster url walks\")\n",
    "print(\"I want to cluster urls/url walks from a given domain, to 3 categories\")\n",
    "print(\"positive correlation, zero correlation, negative correlation, to_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Basic features ( Domain Counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_counts = get_domain_counts(db_df)\n",
    "del db_df\n",
    "pivot_matrix = domain_counts.pivot(index='Domain_Name', columns='Device_ID', values='count').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user use fraction instead of count\n"
     ]
    }
   ],
   "source": [
    "print(\"user use fraction instead of count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device_ID</th>\n",
       "      <th>Domain_Name</th>\n",
       "      <th>Target</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124</td>\n",
       "      <td>3930</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>4136</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>6450</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124</td>\n",
       "      <td>12837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>17665</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288003</th>\n",
       "      <td>69967</td>\n",
       "      <td>2361028</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288004</th>\n",
       "      <td>69967</td>\n",
       "      <td>2387835</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288005</th>\n",
       "      <td>69967</td>\n",
       "      <td>2389761</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288006</th>\n",
       "      <td>69967</td>\n",
       "      <td>2390487</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288007</th>\n",
       "      <td>69967</td>\n",
       "      <td>2408371</td>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288008 rows x 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Device_ID  Domain_Name  Target  count\n",
       "0             124         3930       0      4\n",
       "1             124         4136       0      3\n",
       "2             124         6450       0      3\n",
       "3             124        12837       0      1\n",
       "4             124        17665       0      2\n",
       "...           ...          ...     ...    ...\n",
       "288003      69967      2361028       1      4\n",
       "288004      69967      2387835       1     34\n",
       "288005      69967      2389761       1     72\n",
       "288006      69967      2390487       1      1\n",
       "288007      69967      2408371       1    385\n",
       "\n",
       "[288008 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile correlation.py\n",
    "import numpy as np\n",
    "import ray\n",
    "@ray.remote\n",
    "def calculate_chunk_covariance(matrix_centered, start_idx, end_idx, columns):\n",
    "    # Calculate covariance for this chunk\n",
    "    chunk = matrix_centered[:, start_idx:end_idx]\n",
    "    chunk_cov = np.dot(chunk.T, matrix_centered) / (matrix_centered.shape[0] - 1)\n",
    "    \n",
    "    return start_idx, end_idx, chunk_cov\n",
    "def compute_chunked_covariance(pivot_matrix, batch_size=2000):\n",
    "    # Convert to numpy array for faster computation\n",
    "    matrix_dense = pivot_matrix.to_numpy()\n",
    "    matrix_centered = matrix_dense - np.mean(matrix_dense, axis=0)\n",
    "\n",
    "    # Initialize parameters\n",
    "    n_cols = pivot_matrix.shape[1]\n",
    "    futures = []\n",
    "\n",
    "    # Submit tasks to Ray\n",
    "    for i in range(0, n_cols, batch_size):\n",
    "        batch_end = min(i + batch_size, n_cols)\n",
    "        futures.append(calculate_chunk_covariance.remote(matrix_centered, i, batch_end, pivot_matrix.columns))\n",
    "\n",
    "    # Collect results and combine\n",
    "    cov_chunks = []\n",
    "    for future in ray.get(futures):\n",
    "        start_idx, end_idx, chunk_cov = future\n",
    "        chunk_df = mpd.DataFrame(\n",
    "            chunk_cov,\n",
    "            index=pivot_matrix.columns[start_idx:end_idx],\n",
    "            columns=pivot_matrix.columns\n",
    "        )\n",
    "        cov_chunks.append(chunk_df)\n",
    "\n",
    "    # Combine all chunks\n",
    "    return mpd.concat(cov_chunks)\n",
    "def melt_covariance_matrix(covariance_matrix):\n",
    "    # Reset index to make it a column\n",
    "    melted = covariance_matrix.reset_index()\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    melted = melted.melt(\n",
    "        id_vars=['Device_ID'],\n",
    "        var_name='target',\n",
    "        value_name='covariance'\n",
    "    )\n",
    "    \n",
    "    # Rename the 'index' column to 'source'\n",
    "    melted = melted.rename(columns={'Device_ID': 'source'})\n",
    "    \n",
    "    # Remove duplicate pairs (e.g., if A->B exists, remove B->A)\n",
    "    melted = melted[melted['source'] < melted['target']]\n",
    "    \n",
    "    # Remove rows where source equals target\n",
    "    melted = melted[melted['source'] != melted['target']]\n",
    "    \n",
    "    return melted.reset_index(drop=True)\n",
    "def filter_and_transform_covariance(melted_covariance, abs_threshold=0.03, log_threshold=10):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result = melted_covariance\n",
    "    # Create corrected_cov column\n",
    "    result['corrected_cov'] = result['covariance'].copy()\n",
    "    \n",
    "    # Apply absolute threshold filter\n",
    "    result.loc[abs(result['corrected_cov']) < abs_threshold, 'corrected_cov'] = 0\n",
    "    \n",
    "    # Apply log transformation for values above log_threshold\n",
    "    high_vals_mask = abs(result['corrected_cov']) > log_threshold\n",
    "    result.loc[high_vals_mask, 'corrected_cov'] = result.loc[high_vals_mask, 'corrected_cov'].apply(\n",
    "        lambda x: np.log(abs(x)) * np.sign(x)\n",
    "    )\n",
    "    print(high_vals_mask.mean())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Covariance/Correlation between domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996048833452504\n",
      "Shapes:\n",
      "Corrected covariance: (188805, 4)\n",
      "Device fractions: (288008, 5)\n"
     ]
    }
   ],
   "source": [
    "# Get domain counts and create pivot matrix\n",
    "\n",
    "# Compute covariance matrix and melt it\n",
    "covariance_matrix = compute_chunked_covariance(pivot_matrix)\n",
    "melted_cov = melt_covariance_matrix(covariance_matrix)\n",
    "\n",
    "# Apply filtering and transformation to covariance\n",
    "corrected_cov = filter_and_transform_covariance(melted_cov)\n",
    "\n",
    "# Get device domain fractions from original domain counts\n",
    "device_fractions = get_device_domain_fractions(domain_counts)\n",
    "# Compute domain-target correlations\n",
    "domain_correlations = compute_domain_target_correlation(domain_counts)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(f\"Corrected covariance: {corrected_cov.shape}\")\n",
    "print(f\"Device fractions: {device_fractions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph_model_funcs.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "def create_graph_data(corrected_cov):\n",
    "    # Convert domain names to numerical indices\n",
    "    unique_domains = pd.concat([corrected_cov['source'], corrected_cov['target']]).unique()\n",
    "    domain_to_idx = {domain: idx for idx, domain in enumerate(unique_domains)}\n",
    "    \n",
    "    # Create edge index and edge weights\n",
    "    edge_index = torch.tensor([\n",
    "        [domain_to_idx[s] for s in corrected_cov['source']],\n",
    "        [domain_to_idx[t] for t in corrected_cov['target']]\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    edge_weight = torch.tensor(corrected_cov['corrected_cov'].values, dtype=torch.float)\n",
    "    \n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weight,\n",
    "        num_nodes=len(unique_domains)\n",
    "    )\n",
    "    return data, domain_to_idx\n",
    "\n",
    "def train_node2vec(data, device='cuda', epochs=100):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = Node2Vec(\n",
    "        data.edge_index,\n",
    "        embedding_dim=128,\n",
    "        walk_length=20,\n",
    "        context_size=10,\n",
    "        walks_per_node=10,\n",
    "        p=1,\n",
    "        q=1,\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}')\n",
    "            \n",
    "    return model\n",
    "@ray.remote\n",
    "def _compute_device_embedding(device_id, group, embeddings, domain_mapping):\n",
    "    valid_embeddings = []\n",
    "    valid_weights = []\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        domain = row['Domain_Name']\n",
    "        if domain in domain_mapping:\n",
    "            idx = domain_mapping[domain]\n",
    "            valid_embeddings.append(embeddings[idx])\n",
    "            valid_weights.append(row['fraction'])\n",
    "    \n",
    "    if valid_embeddings:\n",
    "        valid_weights = np.array(valid_weights)\n",
    "        valid_weights = valid_weights / valid_weights.sum()\n",
    "        device_embedding = np.average(valid_embeddings, weights=valid_weights, axis=0)\n",
    "        return device_id, device_embedding\n",
    "    return device_id, None\n",
    "\n",
    "def compute_device_embeddings(device_fractions, embeddings, domain_mapping):\n",
    "    # Group device fractions by Device_ID\n",
    "    grouped_fractions = device_fractions.groupby('Device_ID')\n",
    "    \n",
    "    # Create remote tasks\n",
    "    futures = [\n",
    "        _compute_device_embedding.remote(device_id, group, embeddings, domain_mapping)\n",
    "        for device_id, group in grouped_fractions\n",
    "    ]\n",
    "    \n",
    "    # Collect results\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    # Convert results to dictionary\n",
    "    device_embeddings = {\n",
    "        device_id: embedding \n",
    "        for device_id, embedding in results \n",
    "        if embedding is not None\n",
    "    }\n",
    "    \n",
    "    return device_embeddings\n",
    "\n",
    "# Main flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = get_train_test_masks(domain_counts)\n",
    "data, domain_mapping = create_graph_data(corrected_cov)\n",
    "\n",
    "model = train_node2vec(data)\n",
    "embeddings = model().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Test Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
