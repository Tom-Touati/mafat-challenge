{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fresh Idea\n",
    "## separate one/zero activity of domains\n",
    "- replace zeros by minus one\n",
    "- calculate the class activity for 3 hours bins for each domain\n",
    "- calculate the user activity for gaussian around center of 3 hour bins\n",
    "- calculate the likelihood of the person being a 1/-1 in that time \n",
    "- add user general metrics including domain cls activity and usage patterns\n",
    "\n",
    "### questions\n",
    "- how to take into account times when the person used a website when others didnt?\n",
    "- how to give likelihood when the person didn't show any nearby activity?\n",
    "- what if he used a similar website at same time but more nich? \n",
    "- how to average the bins weighted by the significance of that bin?\n",
    "- how to give weight to the magnitude of number of users entering? probability of 1 with confidence\n",
    "- what about sparse websites?\n",
    "- how to not let times where there are no activity take a lot of weight?\n",
    "### enhancements\n",
    "- create graph embedding of urls\n",
    "- for each bin, calculate the metric per url\n",
    "- instead of only looking at the specific website, take into account websites with similar usages,\n",
    "  for example looking at same domain_cls usage in gaussian around bin, or looking at domain embeddings and looking at the activity in similar embeddings weighted by the distance in the embedding space\n",
    "\n",
    "### NOTICE:\n",
    "the data itself will use all domains, even ones that the person never used. this could be an issue. \n",
    "first of all the fact that the person doesnt use them is an indication. we \n",
    "- we might want to take the niche websites and sum them up\n",
    "- we might want to remove them\n",
    "\n",
    "IDEA!\n",
    "- use different features for different people\n",
    "- make an ensemble that can differentiate between different users\n",
    "- take the people that get a wrong prediction and see if a classifier that is more \"fringe\" can classify them better\n",
    "- for example another tree classifier that takes a smaller amount of features to give more opportunity to fringe websites\n",
    "can create a classifier for each user type \n",
    "can take number of usages for each domain, and cluster people or PCA\n",
    "clustering is good - I can create a classifier for each cluster, from each cluster take all of the available data for all of the visited domains, and create a classifier for them. use only data from those users or all users that used one of the websites, plus the general model, for each cluster - use cluster model and general model.\n",
    "also - I can multiply the features by the log of usages\n",
    "\n",
    "another idea is to simply average the most prominent websites weighted by the specific user usages, and the general usage\n",
    "\n",
    "USER CLUSTER AS FEATURE - or PCA coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 20:38:29,074\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPU': 1.0, 'node:192.168.133.140': 1.0, 'memory': 26758295450.0, 'CPU': 10.0, 'object_store_memory': 5000000000.0, 'node:__internal_head__': 1.0, 'accelerator_type:T500': 1.0}\n",
      "{'__name__': 'modin.config', '__doc__': 'Module houses config entities which can be used for Modin behavior tuning.', '__package__': 'modin.config', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f9033e3ec80>, '__spec__': ModuleSpec(name='modin.config', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f9033e3ec80>, origin='/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config/__init__.py', submodule_search_locations=['/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config']), '__path__': ['/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config'], '__file__': '/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config/__init__.py', '__cached__': '/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config/__pycache__/__init__.cpython-310.pyc', '__builtins__': {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f905bc09e10>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'MemoryError': <class 'MemoryError'>, 'BufferError': <class 'BufferError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
      "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x7f90582cdb40>, 'runfile': <function runfile at 0x7f905815b520>, '__IPYTHON__': True, 'display': <function display at 0x7f905ca6add0>, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f9042e96b30>>}, 'pubsub': <module 'modin.config.pubsub' from '/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config/pubsub.py'>, 'envvars': <module 'modin.config.envvars' from '/home/tompouce/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/config/envvars.py'>, 'AsvDataSizeConfig': <class 'modin.config.envvars.AsvDataSizeConfig'>, 'AsvImplementation': <class 'modin.config.envvars.AsvImplementation'>, 'AsyncReadMode': <class 'modin.config.envvars.AsyncReadMode'>, 'BenchmarkMode': <class 'modin.config.envvars.BenchmarkMode'>, 'CIAWSAccessKeyID': <class 'modin.config.envvars.CIAWSAccessKeyID'>, 'CIAWSSecretAccessKey': <class 'modin.config.envvars.CIAWSSecretAccessKey'>, 'CpuCount': <class 'modin.config.envvars.CpuCount'>, 'DaskThreadsPerWorker': <class 'modin.config.envvars.DaskThreadsPerWorker'>, 'DocModule': <class 'modin.config.envvars.DocModule'>, 'DynamicPartitioning': <class 'modin.config.envvars.DynamicPartitioning'>, 'Engine': <class 'modin.config.envvars.Engine'>, 'EnvironmentVariable': <class 'modin.config.envvars.EnvironmentVariable'>, 'GithubCI': <class 'modin.config.envvars.GithubCI'>, 'GpuCount': <class 'modin.config.envvars.GpuCount'>, 'IsDebug': <class 'modin.config.envvars.IsDebug'>, 'IsExperimental': <class 'modin.config.envvars.IsExperimental'>, 'IsRayCluster': <class 'modin.config.envvars.IsRayCluster'>, 'LazyExecution': <class 'modin.config.envvars.LazyExecution'>, 'LogFileSize': <class 'modin.config.envvars.LogFileSize'>, 'LogMemoryInterval': <class 'modin.config.envvars.LogMemoryInterval'>, 'LogMode': <class 'modin.config.envvars.LogMode'>, 'Memory': <class 'modin.config.envvars.Memory'>, 'MinColumnPartitionSize': <class 'modin.config.envvars.MinColumnPartitionSize'>, 'MinPartitionSize': <class 'modin.config.envvars.MinPartitionSize'>, 'MinRowPartitionSize': <class 'modin.config.envvars.MinRowPartitionSize'>, 'ModinNumpy': <class 'modin.config.envvars.ModinNumpy'>, 'NativeDataframeMode': <class 'modin.config.envvars.NativeDataframeMode'>, 'NPartitions': <class 'modin.config.envvars.NPartitions'>, 'PersistentPickle': <class 'modin.config.envvars.PersistentPickle'>, 'ProgressBar': <class 'modin.config.envvars.ProgressBar'>, 'RangePartitioning': <class 'modin.config.envvars.RangePartitioning'>, 'RayInitCustomResources': <class 'modin.config.envvars.RayInitCustomResources'>, 'RayRedisAddress': <class 'modin.config.envvars.RayRedisAddress'>, 'RayRedisPassword': <class 'modin.config.envvars.RayRedisPassword'>, 'RayTaskCustomResources': <class 'modin.config.envvars.RayTaskCustomResources'>, 'ReadSqlEngine': <class 'modin.config.envvars.ReadSqlEngine'>, 'StorageFormat': <class 'modin.config.envvars.StorageFormat'>, 'TestDatasetSize': <class 'modin.config.envvars.TestDatasetSize'>, 'TestReadFromPostgres': <class 'modin.config.envvars.TestReadFromPostgres'>, 'TestReadFromSqlServer': <class 'modin.config.envvars.TestReadFromSqlServer'>, 'TrackFileLeaks': <class 'modin.config.envvars.TrackFileLeaks'>, 'Parameter': <class 'modin.config.pubsub.Parameter'>, 'ValueSource': <enum 'ValueSource'>, 'context': <function context at 0x7f9011d20040>, '__all__': ['EnvironmentVariable', 'Parameter', 'ValueSource', 'context', 'IsDebug', 'Engine', 'StorageFormat', 'CpuCount', 'GpuCount', 'Memory', 'NativeDataframeMode', 'IsRayCluster', 'RayRedisAddress', 'RayRedisPassword', 'RayInitCustomResources', 'RayTaskCustomResources', 'LazyExecution', 'DaskThreadsPerWorker', 'NPartitions', 'MinPartitionSize', 'MinRowPartitionSize', 'MinColumnPartitionSize', 'TestDatasetSize', 'AsvImplementation', 'AsvDataSizeConfig', 'ProgressBar', 'BenchmarkMode', 'PersistentPickle', 'ModinNumpy', 'RangePartitioning', 'AsyncReadMode', 'ReadSqlEngine', 'IsExperimental', 'DynamicPartitioning', 'TrackFileLeaks', 'TestReadFromSqlServer', 'TestReadFromPostgres', 'GithubCI', 'CIAWSSecretAccessKey', 'CIAWSAccessKeyID', 'LogMode', 'LogMemoryInterval', 'LogFileSize', 'DocModule']}\n"
     ]
    }
   ],
   "source": [
    "!export NEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiMGMyZjIyZC0xMjQzLTQxNjQtYjZjZC0wMTRiZmJmZmRlZjYifQ==\"\n",
    "import os\n",
    "import ray\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "from modin.config import NPartitions,RangePartitioning\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiMGMyZjIyZC0xMjQzLTQxNjQtYjZjZC0wMTRiZmJmZmRlZjYifQ==\"\n",
    "    # !export MODIN_CPUS=2\n",
    "n_cpus = 8\n",
    "plasma_store_size = 5*10**9.\n",
    "os.environ[\"MODIN_CPUS\"] = str(n_cpus)\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "os.environ[\"MODIN_NPARTITIONS\"] = \"12\"\n",
    "os.environ[\"MODIN_RANGE_PARTITIONING\"] = \"True\"\n",
    "ray.init(num_cpus=10, ignore_reinit_error=True, object_store_memory=plasma_store_size)#,_memory=65000000000)\n",
    "print(ray.cluster_resources())\n",
    "import modin.pandas as mpd\n",
    "from modin import config as cfg\n",
    "print(vars(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/tom.touati/web-segmentation/e/WEB-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: Info (NVML): Unknown Error. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/help/nvml_error/\n"
     ]
    }
   ],
   "source": [
    "# !export MODIN_CPUS=2\n",
    "import neptune\n",
    "import os\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"tom.touati/web-segmentation\",  # replace with your project\n",
    "    api_token=os.environ[\"NEPTUNE_API_TOKEN\"],\n",
    "    # name=\"Activity-Based Features\",\n",
    "    capture_stdout=True,\n",
    "    capture_stderr=True,\n",
    "    capture_hardware_metrics=True,\n",
    "    tags=[\"time-based-models\", \"activity-based-features\"],\n",
    "    description=\"User activity patterns analysis\"\n",
    ")\n",
    "import neptune\n",
    "\n",
    "# Initialize Neptune run\n",
    "\n",
    "# Log parameters\n",
    "params = {\n",
    "    \"training_data\":{\n",
    "        \"min_domain_devices\": 10,\n",
    "        \"n_devices_hist\":True,\n",
    "        \"test_size\": 0.2,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \"user_activity_timeseries\": {\n",
    "        \"bin_hours\": 3,\n",
    "        \"gaussian_filter\": True, \n",
    "        \"n_days_each_side\": 3,\n",
    "        \"std\": 1.5,\n",
    "        \"drop_na\": True,\n",
    "        \"drop_zeros\": False\n",
    "    },\n",
    "    \"domain_activity_timeseries\": {\n",
    "        \"bin_hours\": 6,\n",
    "        \"gaussian_filter\": True, \n",
    "        \"n_days_each_side\": 3,\n",
    "        \"std\": 1.5,\n",
    "        \"drop_na\": True,\n",
    "        \"drop_zeros\": False\n",
    "    },\n",
    "    \"general_user_time_bin\": {\n",
    "        \"should_run\": True,\n",
    "        \"bin_hours\": 3\n",
    "    },\n",
    "    \"feature_selection\": {\n",
    "        \"n_features\": 1000,\n",
    "        \"step\": 20000\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 150,\n",
    "        \"max_depth\": 6\n",
    "    }\n",
    "}\n",
    "\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "# Log notebook\n",
    "run[\"notebook\"].upload(\"time_based_model.ipynb\")\n",
    "\n",
    "# Log metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import freeze_support\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  # Modin will use Ray\n",
    "# ray.init()\n",
    "# NPartitions.put(16)\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        # First get 1000 random Device_IDs\n",
    "        selective_device_ids_query = \"\"\"\n",
    "        WITH random_devices AS (\n",
    "            SELECT DISTINCT Device_ID \n",
    "            FROM data \n",
    "            LIMIT 1000\n",
    "        )\n",
    "        SELECT * \n",
    "        FROM data \n",
    "        WHERE Device_ID IN (SELECT Device_ID FROM random_devices)\n",
    "        AND Domain_Name != 1732927\n",
    "        \"\"\"\n",
    "        device_ids_query = \"\"\"SELECT * from data\n",
    "        WHERE Domain_Name != 1732927 \"\"\"\n",
    "        df = mpd.read_sql(selective_device_ids_query, con,index_col='Datetime',\n",
    "                          parse_dates=['Datetime'],\n",
    "                              dtype={\n",
    "                            \"Domain_Name\": \"int32\",\n",
    "                            \"Device_ID\": \"int32\",\n",
    "                            \"Target\": \"int8\"\n",
    "                        },\n",
    "                              columns=[\n",
    "                                  \"Domain_Name\",\n",
    "                                  \"Device_ID\",\n",
    "                                  \"Target\"\n",
    "                              ]\n",
    "                         )._repartition()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 2088 MiB, 49 objects, write throughput 90 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 6522 MiB, 131 objects, write throughput 233 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data: The task's local raylet died. Check raylet.out for more information.\n"
     ]
    },
    {
     "ename": "LocalRayletDiedError",
     "evalue": "The task's local raylet died. Check raylet.out for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocalRayletDiedError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Can use get_connection to get underlying sqlalchemy engine\u001b[39;00m\n\u001b[1;32m      8\u001b[0m conn\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[0;32m----> 9\u001b[0m db_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(db_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m conn  \n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mload_data_from_db\u001b[0;34m(con)\u001b[0m\n\u001b[1;32m     15\u001b[0m     selective_device_ids_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    WITH random_devices AS (\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m        SELECT DISTINCT Device_ID \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    AND Domain_Name != 1732927\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     device_ids_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSELECT * from data\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    WHERE Domain_Name != 1732927 \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mmpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselective_device_ids_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDomain_Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDevice_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDomain_Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDevice_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_repartition()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/logging/logger_decorator.py:144\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    147\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/pandas/io.py:646\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    641\u001b[0m     df_gen \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         ModinObjects\u001b[38;5;241m.\u001b[39mDataFrame(query_compiler\u001b[38;5;241m=\u001b[39mFactoryDispatcher\u001b[38;5;241m.\u001b[39mfrom_pandas(df))\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m df_gen\n\u001b[1;32m    645\u001b[0m     )\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModinObjects\u001b[38;5;241m.\u001b[39mDataFrame(query_compiler\u001b[38;5;241m=\u001b[39m\u001b[43mFactoryDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/core/execution/dispatching/factories/dispatcher.py:272\u001b[0m, in \u001b[0;36mFactoryDispatcher.read_sql\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;129m@_inherit_docstrings\u001b[39m(factories\u001b[38;5;241m.\u001b[39mBaseFactory\u001b[38;5;241m.\u001b[39m_read_sql)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_sql\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/core/execution/dispatching/factories/factories.py:382\u001b[0m, in \u001b[0;36mBaseFactory._read_sql\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m    376\u001b[0m     _doc_io_method_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_sql\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/logging/logger_decorator.py:144\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    147\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/core/io/file_dispatcher.py:159\u001b[0m, in \u001b[0;36mFileDispatcher.read\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mRead data according passed `args` and `kwargs`.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mpostprocessing work on the resulting query_compiler object.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     query_compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ModinAssumptionError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    161\u001b[0m     param_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_or_buf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_or_buf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfname\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/logging/logger_decorator.py:144\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    147\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/core/io/sql/sql_dispatcher.py:124\u001b[0m, in \u001b[0;36mSQLDispatcher._read\u001b[0;34m(cls, sql, con, index_col, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     new_index \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mRangeIndex(\u001b[38;5;28msum\u001b[39m(index_lens))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# concat index returned from partitions\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     index_lst \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 124\u001b[0m         x \u001b[38;5;28;01mfor\u001b[39;00m part_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m part_index\n\u001b[1;32m    125\u001b[0m     ]\n\u001b[1;32m    126\u001b[0m     new_index \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mIndex(index_lst)\u001b[38;5;241m.\u001b[39mset_names(index_col)\n\u001b[1;32m    127\u001b[0m new_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mframe_cls(np\u001b[38;5;241m.\u001b[39marray(partition_ids), new_index, cols_names)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/modin/core/execution/ray/common/engine_wrapper.py:139\u001b[0m, in \u001b[0;36mRayWrapper.materialize\u001b[0;34m(cls, obj_id)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mget(obj_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj_id, ray\u001b[38;5;241m.\u001b[39mObjectRef) \u001b[38;5;28;01melse\u001b[39;00m obj_id\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(obj, ray\u001b[38;5;241m.\u001b[39mObjectRef) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m obj_id):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m ids \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    142\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/ray/_private/worker.py:2771\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2766\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2767\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2768\u001b[0m     )\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2771\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mafat-challenge-UwfjGg_R-py3.10/lib/python3.10/site-packages/ray/_private/worker.py:921\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values, debugger_breakpoint\n",
      "\u001b[0;31mLocalRayletDiedError\u001b[0m: The task's local raylet died. Check raylet.out for more information."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load data from db\n",
    "freeze_support()\n",
    "dbfile = '/home/tompouce/workspaces/mafat-challenge/mafat-challenge/train_data_for_competition/training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())\n",
    "del conn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Device_ID', 'Datetime', 'URL', 'Domain_Name', 'Domain_cls1',\n",
       "       'Domain_cls2', 'Domain_cls3', 'Domain_cls4', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "import matplotlib.pyplot as plt\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_devices(device_target_df, test_size=0.2, random_state=42):    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    return train_device_ids, test_device_ids\n",
    "\n",
    "def get_initial_train_data(db_df, test_size=0.2, random_state=42, min_domain_devices=10,n_devices_hist=False):\n",
    "    device_targets = db_df.groupby(\"Device_ID\")[\"Target\"].first().reset_index()\n",
    "    train_devices, test_device_ids = get_train_test_devices(device_targets,test_size=test_size,random_state=random_state)\n",
    "    train_df = db_df[db_df[\"Device_ID\"].isin(train_devices)]\n",
    "    devices_per_domain = train_df.groupby(\"Domain_Name\")[\"Device_ID\"].nunique()\n",
    "    \n",
    "    domain_mask = devices_per_domain>min_domain_devices\n",
    "    print(f\"Percentage of domains with more than {min_domain_devices} devices: {domain_mask.mean()*100:.2f}%\")\n",
    "    devices_per_domain = devices_per_domain[domain_mask]\n",
    "    if n_devices_hist:\n",
    "        hist = devices_per_domain.hist()\n",
    "        run[\"plots/domain_devices_hist\"].upload(neptune.types.File.as_image(hist.figure))\n",
    "        plt.show()\n",
    "    train_df = train_df[train_df[\"Domain_Name\"].isin(devices_per_domain.index)]\n",
    "    return train_df,train_devices,test_device_ids, device_targets, devices_per_domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,train_devices,test_device_ids, device_targets,active_domains = get_initial_train_data(db_df,**params[\"training_data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess timeseries\n",
    "from functools import partial\n",
    "def process_activity_timeseries(domain_df,bin_hours=6,gaussian_filter=True,n_days_each_side=3,std=1.5,drop_na=True,drop_zeros=False):\n",
    "    activity_per_3h = domain_df[\"Device_ID\"].resample(f'{str(bin_hours)}h').nunique()\n",
    "    gaussian_window_hours = int(n_days_each_side*24/bin_hours*2) # n_days_each_side * 24h / 3h_per_bin * 2 sides\n",
    "    if gaussian_filter:\n",
    "        activity_per_3h = activity_per_3h.rolling(window=gaussian_window_hours, win_type='gaussian',center=True,min_periods=1,closed=\"both\").mean(std=std)\n",
    "    if drop_na:\n",
    "        activity_per_3h.dropna(inplace=True)\n",
    "    if drop_zeros:\n",
    "        activity_per_3h = activity_per_3h[activity_per_3h!=0]\n",
    "    activity_per_3h.rename(\"Activity\",inplace=True)\n",
    "    return activity_per_3h.round().astype(int)\n",
    "\n",
    "def get_domain_activity_timeseries(train_df,domain_ts_kwargs):\n",
    "    process_domain_timeseries = partial(process_activity_timeseries,**domain_ts_kwargs)\n",
    "    process_domain_timeseries.__name__ =process_activity_timeseries.__name__\n",
    "    domain_timeseries = train_df.groupby([\"Domain_Name\",\"Target\"]).apply(process_domain_timeseries)\n",
    "    \n",
    "    domain_fraction_ts = domain_timeseries\n",
    "    del domain_timeseries\n",
    "    domain_fraction_ts[\"activity_fraction\"] = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"]).transform(lambda x: x/x.sum())\n",
    "    # Add the sum of activity as a new column\n",
    "    domain_activity = domain_fraction_ts.groupby([\"Domain_Name\", \"Target\"])[[\"Activity\"]].sum()\n",
    "\n",
    "    domain_activity = domain_activity.rename(columns={\"Activity\": \"target_domain_activity\"})\n",
    "    # Merge the results\n",
    "    domain_fraction_ts = domain_fraction_ts.merge(domain_activity, left_index=True, right_index=True)\n",
    "    # Reset index to get Target as a column, then pivot to get Target as columns\n",
    "    pivot_fraction_ts = domain_fraction_ts.reset_index().pivot(\n",
    "        index=['Datetime', 'Domain_Name'],\n",
    "        columns='Target'\n",
    "    ).fillna(0)\n",
    "    pivot_fraction_ts.columns = [f'{col[0]}_{col[1]}' for col in pivot_fraction_ts.columns]\n",
    "    return pivot_fraction_ts\n",
    "def get_user_activity_timeseries(db_df,user_ts_kwargs):\n",
    "    process_user_timeseries = partial(process_activity_timeseries,**user_ts_kwargs)\n",
    "    process_user_timeseries.__name__ =process_activity_timeseries.__name__\n",
    "    user_timeseries = db_df.groupby([\"Device_ID\",\"Domain_Name\"]).apply(process_user_timeseries)\n",
    "    return user_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "domain_activity_timeseries = get_domain_activity_timeseries(\n",
    "    train_df, params[\"domain_activity_timeseries\"])\n",
    "user_activity_timeseries = get_user_activity_timeseries(\n",
    "    db_df.loc[db_df[\"Domain_Name\"].isin(active_domains).values], params[\"user_activity_timeseries\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_timeseries.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_timeseries.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.MultiIndex.from_tuples(user_activity_timeseries.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_activity_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def class_probability_score(active, p_active_given_a, p_active_given_b, prior_a=0.5, total_users=100):\n",
    "    \"\"\"\n",
    "    Calculate class probability score with vectorized operations\n",
    "    \n",
    "    Args:\n",
    "        active: Boolean indicating if user was active\n",
    "        p_active_given_a: Probability of activity given class A (0)\n",
    "        p_active_given_b: Probability of activity given class B (1)\n",
    "        prior_a: Prior probability for class A\n",
    "        total_users: Total number of users for confidence calculation\n",
    "    \"\"\"\n",
    "    # Use numpy for vectorized operations\n",
    "    likelihood_a = np.where(active, p_active_given_a, 1 - p_active_given_a)\n",
    "    likelihood_b = np.where(active, p_active_given_b, 1 - p_active_given_b)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    evidence = (likelihood_a * prior_a + likelihood_b * (1 - prior_a))\n",
    "    posterior_a = (likelihood_a * prior_a) / evidence\n",
    "\n",
    "    return posterior_a #* confidence_factor\n",
    "\n",
    "def get_user_domain_scores(domain_activity_timeseries,user_activity_timeseries):\n",
    "    merged_timeseries_df = domain_activity_timeseries.merge(\n",
    "        user_activity_timeseries.reset_index(), how=\"left\", on=[\"Domain_Name\", \"Datetime\"]\n",
    "    ).set_index([\"Datetime\", \"Domain_Name\", \"Device_ID\"])\n",
    "\n",
    "    merged_timeseries_df[\"bin_activity\"] = merged_timeseries_df[\"Activity_0\"]+merged_timeseries_df[\"Activity_1\"]\n",
    "    merged_timeseries_df[\"total_activity\"] = (merged_timeseries_df[\"target_domain_activity_0\"]+merged_timeseries_df[\"target_domain_activity_1\"])\n",
    "    merged_timeseries_df[\"relative_0_activity\"] = merged_timeseries_df[\"target_domain_activity_0\"]/merged_timeseries_df[\"total_activity\"]\n",
    "    merged_timeseries_df[\"score\"]=class_probability_score(merged_timeseries_df[\"Activity\"], merged_timeseries_df[\"activity_fraction_0\"], merged_timeseries_df[\"activity_fraction_1\"], prior_a=merged_timeseries_df[\"relative_0_activity\"], total_users=merged_timeseries_df[\"bin_activity\"])\n",
    "    merged_timeseries_df[\"weighted_score\"] = (merged_timeseries_df[\"score\"])*(merged_timeseries_df[\"bin_activity\"])#np.log(1+merged_df[\"bin_activity\"]).astype(int))\n",
    "    final_scores = merged_timeseries_df.groupby([\"Device_ID\",\"Domain_Name\"])[\"weighted_score\"].mean()\n",
    "    final_scores_pivot = final_scores.to_frame().reset_index().pivot(index=\"Device_ID\",columns=\"Domain_Name\").fillna(0)\n",
    "\n",
    "    final_scores_pivot=(final_scores_pivot-final_scores_pivot.values.min())/(final_scores_pivot.values.max()-final_scores_pivot.values.min())*2-1\n",
    "    return final_scores_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot = get_user_domain_scores(domain_activity_timeseries,user_activity_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline features\n",
    "def get_active_days_per_user(user_domain_ts):\n",
    "    \"\"\"\n",
    "    Calculate the number of unique days each user had any activity.\n",
    "\n",
    "    Args:\n",
    "        user_domain_ts: MultiIndex Series with levels [Domain_Name, Device_ID, Datetime]\n",
    "\n",
    "    Returns:\n",
    "        Series with index Device_ID and values being number of unique active days\n",
    "    \"\"\"\n",
    "    # Reset index to get Datetime as a column\n",
    "    df = user_domain_ts.reset_index()\n",
    "\n",
    "    # Convert Datetime to date (removing time component)\n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "\n",
    "    # Group by Device_ID and count unique dates where Activity > 0\n",
    "    active_days = df[df['Activity'] > 0].groupby('Device_ID')['Date'].nunique()\n",
    "\n",
    "    active_days = active_days.astype(int)\n",
    "    active_days.name = \"Active_Days\"\n",
    "    active_days = (active_days-active_days.min()\n",
    "                   ) / (active_days.max()-active_days.min())*2-1\n",
    "    return active_days\n",
    "\n",
    "\n",
    "def get_activity_per_time_bin(df, bin_hours=3):\n",
    "    # Convert datetime to time only\n",
    "    # time_index = db_df.index.to_series().dt.time\n",
    "    # df[\"time\"] = time_index\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"time\"] = db_df.index.to_series().dt.hour.astype(int)//bin_hours\n",
    "    df_copy[\"day_part_activity\"] = 0\n",
    "    activity_per_time_range = df_copy[[\"Device_ID\", \"time\", \"day_part_activity\"]].groupby(\n",
    "        [\"Device_ID\", \"time\"]).count()\n",
    "    activity_per_time_range[\"activity_fraction\"] = activity_per_time_range.groupby(\"Device_ID\").apply(lambda x: x/x.sum()).values\n",
    "    activity_per_time_range = activity_per_time_range[[\"activity_fraction\"]].reset_index()\n",
    "    activity_per_time_range = activity_per_time_range.pivot(index=\"Device_ID\",columns=\"time\",values=\"activity_fraction\")\n",
    "    activity_per_time_range.columns = [f\"time_{col}\" for col in activity_per_time_range.columns]\n",
    "    activity_per_time_range = (activity_per_time_range-activity_per_time_range.stack().min())/(activity_per_time_range.stack().max()-activity_per_time_range.stack().min())*2-1\n",
    "    activity_per_time_range = activity_per_time_range.fillna(0)\n",
    "    return activity_per_time_range  # .round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_days = get_active_days_per_user(user_activity_timeseries)\n",
    "del user_activity_timeseries\n",
    "activity_per_time_range = get_activity_per_time_bin(db_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = device_targets.set_index(\"Device_ID\").join(final_scores_pivot)\n",
    "if active_days is not None:\n",
    "    final_features = final_features.join(active_days,how=\"left\")\n",
    "if activity_per_time_range is not None:\n",
    "    final_features = final_features.join(activity_per_time_range,how=\"left\")\n",
    "final_features = final_features.fillna(0)\n",
    "final_features.columns = [str(col) for col in final_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def prepare_model_data(final_features,train_devices,test_device_ids):\n",
    "    X_train = final_features[final_features.index.isin(train_devices)].drop('Target', axis=1)\n",
    "    y_train = final_features[final_features.index.isin(train_devices)]['Target']\n",
    "\n",
    "    X_test = final_features.loc[final_features.index.isin(test_device_ids)]\n",
    "    y_test = final_features[final_features.index.isin(test_device_ids)]['Target']\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def train_model(X_train,y_train,X_test,y_test,params):\n",
    "    xgb_reg = xgboost.XGBRegressor(random_state=0, subsample=0.8, colsample_bytree=0.8, learning_rate= 0.1,\n",
    "                               n_estimators= 150, max_depth=6, objective ='binary:logistic' ,eval_metric =roc_auc_score)\n",
    "    selector = RFE(xgb_reg, n_features_to_select=1000, step=20000)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    best_features = list(X_train.columns[selector.support_])\n",
    "    test_prediction = selector.estimator_.predict(X_test[best_features])\n",
    "    test_auc =round(roc_auc_score(y_test,test_prediction), 3)\n",
    "    return test_auc,selector, best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test = prepare_model_data(final_features,train_devices,test_device_ids)\n",
    "score,selector,best_features = train_model(X_train,y_train,X_test,y_test,params[\"model\"])\n",
    "print(f'The auc for validation set: {score}')\n",
    "run[\"metrics/roc_auc\"] = score\n",
    "run[\"metrics/selected_features\"] = best_features\n",
    "run[\"metrics/feature_importances\"] = selector.estimator_.feature_importances_\n",
    "run[\"metrics/feature_ranking\"] = selector.ranking_\n",
    "run[\"metrics/feature_support\"] = selector.support_\n",
    "# Close the run\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual feature filter\n",
    "# Create function to filter features based on mean values\n",
    "# def filter_low_mean_features(features_df, percentile=0.1):\n",
    "#     # Calculate absolute mean values for each feature\n",
    "#     abs_means = abs(features_df).mean()\n",
    "    \n",
    "#     # Calculate percentile threshold of absolute means\n",
    "#     threshold = abs_means.quantile(percentile)\n",
    "    \n",
    "#     # Get features with absolute means above threshold\n",
    "#     significant_features = abs_means[abs_means >= threshold].index\n",
    "    \n",
    "#     # Filter features\n",
    "\n",
    "#     return significant_features\n",
    "\n",
    "# Apply the filtering function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
