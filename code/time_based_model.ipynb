{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fresh Idea\n",
    "## separate one/zero activity of domains\n",
    "- replace zeros by minus one\n",
    "- calculate the class activity for 3 hours bins for each domain\n",
    "- calculate the user activity for gaussian around center of 3 hour bins\n",
    "- calculate the likelihood of the person being a 1/-1 in that time \n",
    "- add user general metrics including domain cls activity and usage patterns\n",
    "\n",
    "### questions\n",
    "- how to take into account times when the person used a website when others didnt?\n",
    "- how to give likelihood when the person didn't show any nearby activity?\n",
    "- what if he used a similar website at same time but more nich? \n",
    "- how to average the bins weighted by the significance of that bin?\n",
    "- how to give weight to the magnitude of number of users entering? probability of 1 with confidence\n",
    "- what about sparse websites?\n",
    "- how to not let times where there are no activity take a lot of weight?\n",
    "### enhancements\n",
    "- create graph embedding of urls\n",
    "- for each bin, calculate the metric per url\n",
    "- instead of only looking at the specific website, take into account websites with similar usages,\n",
    "  for example looking at same domain_cls usage in gaussian around bin, or looking at domain embeddings and looking at the activity in similar embeddings weighted by the distance in the embedding space\n",
    "\n",
    "### NOTICE:\n",
    "the data itself will use all domains, even ones that the person never used. this could be an issue. \n",
    "first of all the fact that the person doesnt use them is an indication. we \n",
    "- we might want to take the niche websites and sum them up\n",
    "- we might want to remove them\n",
    "\n",
    "IDEA!\n",
    "- use different features for different people\n",
    "- make an ensemble that can differentiate between different users\n",
    "- take the people that get a wrong prediction and see if a classifier that is more \"fringe\" can classify them better\n",
    "- for example another tree classifier that takes a smaller amount of features to give more opportunity to fringe websites\n",
    "can create a classifier for each user type \n",
    "can take number of usages for each domain, and cluster people or PCA\n",
    "clustering is good - I can create a classifier for each cluster, from each cluster take all of the available data for all of the visited domains, and create a classifier for them. use only data from those users or all users that used one of the websites, plus the general model, for each cluster - use cluster model and general model.\n",
    "also - I can multiply the features by the log of usages\n",
    "\n",
    "another idea is to simply average the most prominent websites weighted by the specific user usages, and the general usage\n",
    "\n",
    "USER CLUSTER AS FEATURE - or PCA coefficients\n",
    "\n",
    "an idea - see how chaotic are the subject's patterns, if it's too predictable then it might be a bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tom.touati/mafat-challenge/code/submission\")\n",
    "import importlib\n",
    "import os\n",
    "import ray\n",
    "from modin.config import NPartitions,RangePartitioning\n",
    "%matplotlib widget\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiMGMyZjIyZC0xMjQzLTQxNjQtYjZjZC0wMTRiZmJmZmRlZjYifQ==\"\n",
    "    # !export MODIN_CPUS=2\n",
    "# n_cpus = 8\n",
    "plasma_store_size = 170*(1024**3)\n",
    "heap_memory = 200*(1024**3)\n",
    "# os.environ[\"MODIN_CPUS\"] = str(n_cpus)\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "os.environ[\"MODIN_NPARTITIONS\"] = \"30\"\n",
    "os.environ[\"MODIN_RANGE_PARTITIONING\"] = \"True\"\n",
    "# os.environ[\"MODIN_MEMORY\"] = str(plasma_store_size)\n",
    "ray.init(num_cpus =30,ignore_reinit_error=True, object_store_memory=plasma_store_size,_memory=heap_memory)\n",
    "# print(ray.cluster_resources())\n",
    "import modin.pandas as mpd\n",
    "from modin import config as cfg\n",
    "print(vars(cfg))\n",
    "NEPTUNE_MODE=\"sync\"\n",
    "params={}\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/load_and_prepare_input.py\n",
    "import os\n",
    "import sqlite3\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "  # Modin will use Ray\n",
    "# ray.init()\n",
    "# NPartitions.put(16)\n",
    "def load_domain_data_from_db(con,domain_cls=False,only_domain=False):\n",
    "    try:\n",
    "        device_ids_query = f\"\"\"SELECT Datetime,Device_ID,Domain_Name,Target from data\n",
    "        WHERE Domain_Name != 1732927\n",
    "        \"\"\"\n",
    "        \n",
    "        # WHERE Domain_Name != 1732927 \"\"\"\n",
    "        df = mpd.read_sql(device_ids_query, con\n",
    "                         )._repartition()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "def load_cls_data_from_db(con,domain_cls=False,only_domain=False):\n",
    "    try:\n",
    "        device_ids_query = f\"\"\"SELECT Datetime,Device_ID,Domain_cls1,Domain_cls2,Domain_cls3,Domain_cls4,Target\n",
    "        from data\n",
    "        WHERE Domain_Name != 1732927\n",
    "        \"\"\"\n",
    "        df = mpd.read_sql(device_ids_query, con\n",
    "                         )._repartition()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "def load_and_prepare_data(data_type=\"domain\"):\n",
    "    freeze_support()\n",
    "    dbfile = '../../data/training_set.db'\n",
    "\n",
    "    conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "    # Can use get_connection to get underlying sqlalchemy engine\n",
    "    conn.get_connection()\n",
    "    if data_type==\"domain\":\n",
    "        db_df = load_domain_data_from_db(conn)\n",
    "    elif data_type==\"cls\":\n",
    "        db_df = load_cls_data_from_db(conn)\n",
    "    \n",
    "    print(db_df.head())\n",
    "    del conn\n",
    "    db_df['Datetime'] = mpd.to_datetime(db_df['Datetime'])\n",
    "    db_df.set_index('Datetime', inplace=True)\n",
    "    if data_type==\"domain\":\n",
    "        db_df = db_df.astype( {'Domain_Name': 'uint32', 'Device_ID': 'uint32', 'Target': 'uint8'})\n",
    "    elif data_type==\"cls\":\n",
    "        db_df = db_df.astype( {'Device_ID': 'uint32','Domain_cls1': 'uint32','Domain_cls2': 'uint32', 'Domain_cls3': 'uint32', 'Domain_cls4': 'uint32','Target': 'uint32'})\n",
    "    return db_df\n",
    "\n",
    "# prepare training data\n",
    "import matplotlib.pyplot as plt\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_test_devices(device_target_df, test_size=0.2, random_state=43):    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    return train_device_ids, test_device_ids\n",
    "\n",
    "def get_initial_train_data(db_df, test_size=0.2, random_state=42, min_domain_devices=10,n_devices_hist=False):\n",
    "    device_targets = db_df.groupby(\"Device_ID\")[\"Target\"].first().reset_index()\n",
    "    train_devices, test_device_ids = get_train_test_devices(device_targets,test_size=test_size,random_state=random_state)\n",
    "    train_df = db_df[db_df[\"Device_ID\"].isin(train_devices)]\n",
    "    devices_per_domain = train_df.groupby(\"Domain_Name\")[\"Device_ID\"].nunique()\n",
    "    \n",
    "    domain_mask = devices_per_domain>min_domain_devices\n",
    "    print(f\"Percentage of domains with more than {min_domain_devices} devices: {domain_mask.mean()*100:.2f}%\")\n",
    "    devices_per_domain = devices_per_domain[domain_mask]\n",
    "    if n_devices_hist:\n",
    "        hist = devices_per_domain.hist()\n",
    "        # run[\"plots/domain_devices_hist\"].upload(neptune.types.File.as_image(hist.figure))\n",
    "        plt.show()\n",
    "    train_df = train_df[train_df[\"Domain_Name\"].isin(devices_per_domain.index)]\n",
    "    return train_df,train_devices,test_device_ids, device_targets, devices_per_domain\n",
    "\n",
    "# Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission.domain_timeseries_processing import *\n",
    "from submission.utils import *\n",
    "from submission.load_and_prepare_input import *\n",
    "from submission.prepare_and_train_model import *\n",
    "from submission.content_based_features import *\n",
    "from submission.frequency_base_feats import *\n",
    "from submission.cls_features import *\n",
    "\n",
    "db_df = load_and_prepare_data()\n",
    "\n",
    "params.update({\"training_data\": {\n",
    "    \"min_domain_devices\": 10,\n",
    "    \"n_devices_hist\": False,\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42\n",
    "}})\n",
    "train_df, train_devices, test_device_ids, device_targets, devices_per_valid_domain = get_initial_train_data(\n",
    "    db_df, **params[\"training_data\"])\n",
    "train_df = train_df._repartition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile submission/utils.py\n",
    "import modin.pandas as mpd\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "def z_normalize_by_all(df,train_devices,per_column = True,fillval=0,fill_na_pre_transform=False, scaler=None):\n",
    "    if scaler is not None:\n",
    "        df.iloc[:, :] = scaler.transform(\n",
    "            df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "        if fillval:\n",
    "            df.fillna(fillval,inplace=True)\n",
    "        return\n",
    "    scaler = StandardScaler()\n",
    "    train_data = df.loc[train_devices]\n",
    "    scaler.fit(train_data if per_column else train_data.values.reshape(-1, 1))\n",
    "\n",
    "    # Transform all data using fitted scaler\n",
    "    if fill_na_pre_transform:\n",
    "        df.fillna(fillval,inplace=True)\n",
    "    df.iloc[:, :] = scaler.transform(\n",
    "        df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "    if fillval is not None:\n",
    "        df.fillna(fillval,inplace=True)\n",
    "    params = {\n",
    "        \"mean_\": float(scaler.mean_[0]),  # Convert to native Python float\n",
    "        \"var_\": float(scaler.var_[0]),\n",
    "        \"scale_\": float(scaler.scale_[0]),\n",
    "        \"n_samples_seen_\": int(scaler.n_samples_seen_),\n",
    "    }\n",
    "    return params\n",
    "def min_max_scale_all_values(df, train_devices, per_column=False,scaler=None):\n",
    "    if scaler is not None:\n",
    "        df.iloc[:, :] = scaler.transform(\n",
    "            df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "        return\n",
    "    # Create MinMaxScaler and fit on training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_data = df.loc[train_devices]\n",
    "    scaler.fit(train_data if per_column else train_data.values.reshape(-1, 1))\n",
    "\n",
    "    # Transform all data using fitted scaler\n",
    "    df.iloc[:, :] = scaler.transform(\n",
    "        df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "\n",
    "    # Save scaler parameters as JSON\n",
    "    scaler_params = {\n",
    "        \"min_\": float(scaler.min_[0]),  # Convert to native Python float\n",
    "        \"scale_\": float(scaler.scale_[0]),\n",
    "        \"data_min_\": float(scaler.data_min_[0]),\n",
    "        \"data_max_\": float(scaler.data_max_[0]),\n",
    "        \"data_range_\": float(scaler.data_range_[0]),\n",
    "        # Convert tuple to list for JSON\n",
    "        \"feature_range\": list(scaler.feature_range)\n",
    "    }\n",
    "    return scaler_params\n",
    "\n",
    "from sklearn.impute import KNNImputer# , IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from scipy.sparse import csr_matrix\n",
    "# from implicit.cpu.als import AlternatingLeastSquares\n",
    "def impute_missing_values(final_scores_pivot,train_device_ids):\n",
    "    # Use SoftImpute to fill missing values\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    imputer.fit(final_scores_pivot.loc[train_device_ids])\n",
    "    imputed_scores = imputer.transform(final_scores_pivot)\n",
    "    # imputer = AlternatingLeastSquares(factors=10, regularization=0.01, iterations=10,random_state=0)\n",
    "    # # imputer = MissForest(max_depth=6,max_features=0.8, random_state=0)\n",
    "    # imputer.fit( user_items = csr_matrix(final_scores_pivot.loc[train_device_ids].values))\n",
    "    \n",
    "    # imputed_scores = imputer.recommend_all(csr_matrix(final_scores_pivot.values))\n",
    "    return imputed_scores\n",
    "import gc\n",
    "import ctypes\n",
    "import sys\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Force cleanup of memory by:\n",
    "    1. Running garbage collection\n",
    "    2. Attempting to release memory back to OS\n",
    "    \"\"\"\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Attempt to release memory back to the OS\n",
    "    if sys.platform.startswith('linux'):\n",
    "        libc = ctypes.CDLL('libc.so.6')\n",
    "        # MALLOC_TRIM(0) releases memory back to OS if possible\n",
    "        print(libc.malloc_trim(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Timeseries Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/domain_timeseries_processing.py\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import ray\n",
    "\n",
    "def process_activity_timeseries(domain_df, bin_hours=6, gaussian_filter=True, n_days_each_side=3, std=1.5, drop_na=True, drop_zeros=False):\n",
    "    activity_per_3h = domain_df[[\"Device_ID\"]].resample(\n",
    "        f'{str(bin_hours)}h').nunique()\n",
    "    activity_per_3h.rename(columns={\"Device_ID\": \"Activity\"}, inplace=True)\n",
    "    # activity_per_3h = activity_per_3h.to_frame()\n",
    "\n",
    "    # n_days_each_side * 24h / 3h_per_bin * 2 sides\n",
    "    gaussian_window_hours = int(n_days_each_side*24/bin_hours*2)\n",
    "    if gaussian_filter:\n",
    "        activity_per_3h = activity_per_3h.rolling(\n",
    "            window=gaussian_window_hours, win_type='gaussian', center=True, min_periods=1, closed=\"both\").mean(std=std)\n",
    "    if drop_na:\n",
    "        activity_per_3h.dropna(inplace=True)\n",
    "    if drop_zeros:\n",
    "        activity_per_3h = activity_per_3h[activity_per_3h[\"Activity\"] != 0]\n",
    "    return activity_per_3h.round().astype(int)\n",
    "\n",
    "\n",
    "def _calculate_p1_if_active(domain_activity_timeseries):\n",
    "    domain_activity_timeseries[\"bin_activity\"] = domain_activity_timeseries[\"Activity_0\"] + \\\n",
    "        domain_activity_timeseries[\"Activity_1\"]\n",
    "    domain_activity_timeseries[\"total_domain_activity\"] = (\n",
    "        domain_activity_timeseries[\"target_domain_activity_0\"]+domain_activity_timeseries[\"target_domain_activity_1\"])\n",
    "    domain_activity_timeseries[\"relative_bin_activity\"] = domain_activity_timeseries[\"bin_activity\"] / \\\n",
    "        domain_activity_timeseries[\"total_domain_activity\"]\n",
    "    domain_activity_timeseries[\"p_Active|0\"] = domain_activity_timeseries[\"Activity_0\"]/(\n",
    "        domain_activity_timeseries[\"0_users\"]\n",
    "    )\n",
    "    domain_activity_timeseries[\"p_active|1\"] = domain_activity_timeseries[\"Activity_1\"]/(\n",
    "        domain_activity_timeseries[\"1_users\"])\n",
    "    domain_activity_timeseries[\"p_1\"] = domain_activity_timeseries[\"1_users\"]/(\n",
    "        domain_activity_timeseries[\"0_users\"]+domain_activity_timeseries[\"1_users\"])\n",
    "    domain_activity_timeseries[\"p_active\"] = domain_activity_timeseries[\"bin_activity\"]/(\n",
    "        domain_activity_timeseries[\"0_users\"]+domain_activity_timeseries[\"1_users\"])\n",
    "    domain_activity_timeseries[\"p_1|active\"] = domain_activity_timeseries[\"p_active|1\"]*(\n",
    "        domain_activity_timeseries[\"p_1\"]/domain_activity_timeseries[\"p_active\"])\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_domain_activity_timeseries(train_df, domain_ts_kwargs):\n",
    "    process_domain_timeseries = partial(\n",
    "        process_activity_timeseries, **domain_ts_kwargs)\n",
    "    process_domain_timeseries.__name__ = process_activity_timeseries.__name__\n",
    "    domain_timeseries = train_df[[\"Domain_Name\", \"Target\", \"Device_ID\"]].groupby(\n",
    "        [\"Domain_Name\", \"Target\"]).apply(process_domain_timeseries)\n",
    "    domain_timeseries[\"activity_fraction\"] = domain_timeseries.groupby(\n",
    "        [\"Domain_Name\", \"Target\"]).transform(lambda x: x/x.sum())\n",
    "    # Add the sum of activity as a new column\n",
    "    domain_activity = domain_timeseries.groupby(\n",
    "        [\"Domain_Name\", \"Target\"])[[\"Activity\"]].sum()\n",
    "    domain_activity = domain_activity.rename(\n",
    "        columns={\"Activity\": \"target_domain_activity\"})\n",
    "    # Merge the results\n",
    "    domain_timeseries = domain_timeseries.merge(\n",
    "        domain_activity, left_index=True, right_index=True)\n",
    "    # Reset index to get Target as a column, then pivot to get Target as columns\n",
    "    pivot_fraction_ts = domain_timeseries.reset_index().pivot(\n",
    "        index=['Datetime', 'Domain_Name'],\n",
    "        columns='Target'\n",
    "    ).fillna(0)\n",
    "    pivot_fraction_ts.columns = [\n",
    "        f'{col[0]}_{col[1]}' for col in pivot_fraction_ts.columns]\n",
    "    target_users_per_domain = train_df.groupby([\"Domain_Name\", \"Target\"])[\"Device_ID\"].nunique(\n",
    "    ).unstack().fillna(0).astype(int).rename(columns={0: \"0_users\", 1: \"1_users\"})\n",
    "    pivot_fraction_ts = pivot_fraction_ts.reset_index(\"Datetime\").join(\n",
    "        target_users_per_domain, how=\"left\").set_index(\"Datetime\", append=True).swaplevel()\n",
    "    _calculate_p1_if_active(pivot_fraction_ts)\n",
    "    return pivot_fraction_ts\n",
    "\n",
    "\n",
    "def get_user_activity_timeseries(db_df, user_ts_kwargs):\n",
    "    process_user_timeseries = partial(\n",
    "        process_activity_timeseries, **user_ts_kwargs)\n",
    "    process_user_timeseries.__name__ = process_activity_timeseries.__name__\n",
    "    user_timeseries = db_df[[\"Domain_Name\", \"Device_ID\"]].groupby(\n",
    "        [\"Domain_Name\", \"Device_ID\"]).apply(process_user_timeseries)\n",
    "    # user_timeseries.index = mpd.MultiIndex.from_tuples(user_timeseries.index.to_list(),names=[\"Domain_Name\",\"Device_ID\",\"Datetime\"])\n",
    "    return user_timeseries\n",
    "\n",
    "\n",
    "def get_user_domain_scores(domain_activity_timeseries, user_activity_timeseries):\n",
    "    # Merge domain and user timeseries data more efficiently\n",
    "    merged_timeseries_df = domain_activity_timeseries[[\"p_1|active\", \"bin_activity\"]].reset_index().merge(\n",
    "        user_activity_timeseries.reset_index(),\n",
    "        how=\"inner\",\n",
    "        on=[\"Domain_Name\", \"Datetime\"]\n",
    "    ).set_index([\"Datetime\", \"Domain_Name\", \"Device_ID\"])\n",
    "\n",
    "    # Filter for active periods first to reduce data size\n",
    "    merged_timeseries_df = merged_timeseries_df[merged_timeseries_df[\"Activity\"] > 0]._repartition(\n",
    "    )\n",
    "\n",
    "    # Calculate scores directly\n",
    "    # Calculate relative activity using transform for vectorized operation\n",
    "    group_sums = merged_timeseries_df.groupby(['Domain_Name', 'Device_ID'])[\n",
    "        'bin_activity'].transform('sum')\n",
    "\n",
    "    # Vectorized division\n",
    "    merged_timeseries_df['relative_active_bins_activity'] = merged_timeseries_df['bin_activity'] / group_sums\n",
    "\n",
    "    # Calculate weighted scores in one step\n",
    "    merged_timeseries_df[\"weighted_score\"] = (merged_timeseries_df[\"p_1|active\"]) * \\\n",
    "        (merged_timeseries_df[\"relative_active_bins_activity\"])\n",
    "\n",
    "    # Get final scores with optimized groupby\n",
    "    final_scores = merged_timeseries_df.groupby(\n",
    "        [\"Device_ID\", \"Domain_Name\"])[\"weighted_score\"].sum()\n",
    "\n",
    "    # Create pivot table efficiently\n",
    "    final_scores_pivot = final_scores.unstack()\n",
    "\n",
    "    return final_scores_pivot\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def get_weighted_final_scores(final_scores_pivot,domain_usage_proportion,square_usage=False):\n",
    "    # plt.figure()\n",
    "    mult = domain_usage_proportion[final_scores_pivot.columns\n",
    "        ]\n",
    "    if square_usage:\n",
    "        mult = mult**2\n",
    "    weighted_final_scores = final_scores_pivot.mul(mult)\n",
    "\n",
    "    # target_features = device_targets.set_index(\"Device_ID\").join(weighted_final_scores)\n",
    "    # target_features.set_index(\"Target\")[[int(x) for x in best_features if x.isnumeric()]].stack().groupby(\"Target\").mean().plot(kind=\"bar\")\n",
    "    # plt.show()\n",
    "    return weighted_final_scores\n",
    "    # with open('submission/minmax_scaler.json', 'w') as f:\n",
    "    #     json.dump(scaler_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"domain_activity_timeseries\": {\n",
    "    \"bin_hours\": 6,\n",
    "    \"gaussian_filter\": True,\n",
    "    \"n_days_each_side\": 7,\n",
    "    \"std\": 1.5,\n",
    "    \"drop_na\": False,\n",
    "    \"drop_zeros\": True\n",
    "}})\n",
    "\n",
    "params.update({\"user_activity_timeseries\": {\n",
    "    \"bin_hours\": 6,\n",
    "    \"gaussian_filter\": True,\n",
    "    \"n_days_each_side\": 3,\n",
    "    \"std\": 1.5,\n",
    "    \"drop_na\": True,\n",
    "    \"drop_zeros\": False\n",
    "}})\n",
    "domain_activity_timeseries = get_domain_activity_timeseries(\n",
    "    train_df, params[\"domain_activity_timeseries\"])\n",
    "\n",
    "db_df_valid_mask = db_df[\"Domain_Name\"].isin(devices_per_valid_domain.index)\n",
    "user_activity_timeseries = get_user_activity_timeseries(\n",
    "    db_df.loc[db_df_valid_mask], params[\"user_activity_timeseries\"])\n",
    "\n",
    "p_scores_df = get_user_domain_scores(\n",
    "    domain_activity_timeseries, user_activity_timeseries)\n",
    "p_scores_df -= 0.5\n",
    "print(\"Why minmax here? it possibly creates a bias\")\n",
    "# prob_score_scaler_params = min_max_scale_all_values(p_scores_df, train_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Usage Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/content_based_features.py\n",
    "import numpy as np\n",
    "def get_domain_usage_proportion(db_df):\n",
    "    res =  db_df.groupby(\"Device_ID\")[\"Domain_Name\"].value_counts(normalize=True).unstack(fill_value=0).astype(np.float32)\n",
    "    return res\n",
    "\n",
    "def get_proportion_of_domains_visited(df):\n",
    "    res = df.groupby(\"Device_ID\")[\"Domain_Name\"].nunique()/df[\"Domain_Name\"].nunique()\n",
    "    res.name = \"n_domains\"\n",
    "    return res.to_frame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_usage_proportion = get_domain_usage_proportion(\n",
    "    db_df.loc[db_df[\"Domain_Name\"].isin(devices_per_valid_domain.index)])\n",
    "max_domain_usage = domain_usage_proportion.max(axis=1).to_frame()\n",
    "max_domain_scaler = z_normalize_by_all(\n",
    "    max_domain_usage, train_devices, per_column=True)\n",
    "domain_usage_proportion = np.log(1+domain_usage_proportion)\n",
    "domain_usage_proportion = (\n",
    "    (domain_usage_proportion.T )/domain_usage_proportion.T.max()).T\n",
    "# del cls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Based Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile submission/frequency_base_feats.py\n",
    "import numpy as np\n",
    "import modin.pandas as mpd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_active_days_per_user(user_domain_ts):\n",
    "    \"\"\"\n",
    "    Calculate the number of unique days each user had any activity.\n",
    "\n",
    "    Args:\n",
    "        user_domain_ts: MultiIndex Series with levels [Domain_Name, Device_ID, Datetime]\n",
    "\n",
    "    Returns:\n",
    "        Series with index Device_ID and values being number of unique active days\n",
    "    \"\"\"\n",
    "    # Reset index to get Datetime as a column\n",
    "    df = user_domain_ts.reset_index()\n",
    "\n",
    "    # Convert Datetime to date (removing time component)\n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "\n",
    "    # Group by Device_ID and count unique dates where Activity > 0\n",
    "    active_days = df[df['Activity'] > 0].groupby('Device_ID')['Date'].nunique()\n",
    "\n",
    "    active_days = active_days.astype(int)\n",
    "    active_days.name = \"Active_Days\"\n",
    "    active_days = (active_days-active_days.min()\n",
    "                   ) / (active_days.max()-active_days.min())*2-1\n",
    "    return active_days\n",
    "\n",
    "\n",
    "def get_activity_per_time_bin(df, bin_hours=3):\n",
    "    # Convert datetime to time only\n",
    "    # time_index = db_df.index.to_series().dt.time\n",
    "    # df[\"time\"] = time_index\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"time\"] = db_df.index.to_series().dt.hour.astype(int)//bin_hours\n",
    "    df_copy[\"day_part_activity\"] = 0\n",
    "    activity_per_time_range = df_copy[[\"Device_ID\", \"time\", \"day_part_activity\"]].groupby(\n",
    "        [\"Device_ID\", \"time\"]).count()\n",
    "    activity_per_time_range[\"activity_fraction\"] = activity_per_time_range.groupby(\"Device_ID\").apply(lambda x: x/x.sum()).values\n",
    "    activity_per_time_range = activity_per_time_range[[\"activity_fraction\"]].reset_index()\n",
    "    activity_per_time_range = activity_per_time_range.pivot(index=\"Device_ID\",columns=\"time\",values=\"activity_fraction\")\n",
    "    activity_per_time_range.columns = [f\"time_{col}\" for col in activity_per_time_range.columns]\n",
    "    activity_per_time_range = (activity_per_time_range-activity_per_time_range.stack().min())/(activity_per_time_range.stack().max()-activity_per_time_range.stack().min())*2-1\n",
    "    activity_per_time_range = activity_per_time_range.fillna(0)\n",
    "    return activity_per_time_range  # .round().astype(int)\n",
    "def get_device_domain_pca(user_activity_timeseries, n_components=100):\n",
    "    # Calculate total entries per domain for each device\n",
    "    domain_entries = user_activity_timeseries.reset_index()[['Device_ID', 'Domain_Name','Activity']].groupby(['Device_ID', 'Domain_Name'])['Activity'].sum()\n",
    "    domain_entries_pivot = domain_entries.unstack(fill_value=0)\n",
    "    # domain_entries_pivot = domain_entries_pivot/domain_entries_pivot.sum(axis=1)\n",
    "    # Normalize the data\n",
    "    # normalized_data = (domain_entries_pivot - domain_entries_pivot.min().min()) / (domain_entries_pivot.max().max() - domain_entries_pivot.min().min())\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(domain_entries_pivot.fillna(0))\n",
    "    # Print explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"Explained variance ratio: {explained_variance.sum():.3f}\")\n",
    "    # Create DataFrame with PCA results\n",
    "    pca_df = mpd.DataFrame(\n",
    "        pca_result,\n",
    "        index=domain_entries_pivot.index,\n",
    "        columns=[f'pca_domain_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    return pca_df\n",
    "#activity fft\n",
    "from scipy import fft\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def get_ps_df(db_df):\n",
    "    device_activity_ts = db_df.groupby(\"Device_ID\")[\"Domain_Name\"].resample(\"3H\").count()\n",
    "    device_activity_ts = device_activity_ts.unstack().fillna(0)\n",
    "    # device_activity_ts = StandardScaler().fit_transform(device_activity_ts.T)\n",
    "    power_spectrums = np.abs(fft.rfft(device_activity_ts,axis=1))**2\n",
    "    \n",
    "    sample_d = 3*60*60\n",
    "    \n",
    "    freqs = fft.rfftfreq(device_activity_ts.shape[1],d=sample_d)\n",
    "    freq_mask = freqs>=0\n",
    "    power_spectrums = power_spectrums[:,freq_mask]\n",
    "    freqs = freqs[freq_mask]\n",
    "\n",
    "    psd_df = mpd.DataFrame(power_spectrums,index=device_activity_ts.index,columns=freqs)\n",
    "    # Get power spectra for training devices only\n",
    "\n",
    "    \n",
    "    return psd_df\n",
    "# Convert to dataframe and normalize\n",
    "# power_spectrums_df = power_spectrums.to_frame('power_spectrum')\n",
    "# power_spectrums_df = (power_spectrums_df - power_spectrums_df.min()) / (power_spectrums_df.max() - power_spectrums_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleanup_memory()\n",
    "psd_df = get_ps_df(db_df)\n",
    "psd_df_scaler_params = z_normalize_by_all(\n",
    "    psd_df, train_devices, per_column=True)\n",
    "domains_visited_proportion = get_proportion_of_domains_visited(\n",
    "    db_df[db_df[\"Domain_Name\"].isin(devices_per_valid_domain.index)])\n",
    "domains_visited_scaler_params = z_normalize_by_all(\n",
    "    domains_visited_proportion, train_devices, per_column=True, fillval=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/cls_features.py\n",
    "import numpy as np\n",
    "def get_cls_proportion(df):\n",
    "    cols = [\"Domain_cls1\",\"Domain_cls2\",\"Domain_cls3\",\"Domain_cls4\"]\n",
    "    df = df.set_index(\"Device_ID\")[cols].stack()\n",
    "    df = df[df!=0]\n",
    "    df.index = df.index.droplevel(1)\n",
    "    df = df.groupby(\"Device_ID\").value_counts(normalize=True).unstack().astype(np.float32)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del db_df\n",
    "del train_df\n",
    "del domain_activity_timeseries\n",
    "del user_activity_timeseries\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `Series.groupby_on_multiple_columns` is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    }
   ],
   "source": [
    "# cls_data = load_and_prepare_data(\"cls\")\n",
    "\n",
    "cls_proportion = get_cls_proportion(cls_data)\n",
    "cls_proportion_scaler = z_normalize_by_all(cls_proportion,\n",
    "                                           train_devices,\n",
    "                                           per_column=True,\n",
    "                                           fill_na_pre_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cls_activity_ts(cls_data):\n",
    "#     cls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"weighted_final_scores\": {}})  #\"fillna\": True}})\n",
    "weighted_final_scores = get_weighted_final_scores(\n",
    "    p_scores_df._repartition(), domain_usage_proportion._repartition(),\n",
    "    **params[\"weighted_final_scores\"])\n",
    "scores_scaler = z_normalize_by_all(weighted_final_scores,\n",
    "                                   train_devices,\n",
    "                                   per_column=False,\n",
    "                                   fill_na_pre_transform=False)\n",
    "\n",
    "# col_score_scaler = z_normalize_by_all(weighted_final_scores, train_devices, per_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "weighted_final_scores.stack().hist(bins=1000)\n",
    "plt.xlim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_weighted_final_scores() got an unexpected keyword argument 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_probability_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfillna\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     }\n\u001b[1;32m      7\u001b[0m })\n\u001b[1;32m      8\u001b[0m step_params \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_probability_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m mean_probability_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_weighted_final_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_scores_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdomain_usage_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msquare_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquare_usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfillna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m     14\u001b[0m proportions_used \u001b[38;5;241m=\u001b[39m domain_usage_proportion[p_scores_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_probability_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: get_weighted_final_scores() got an unexpected keyword argument 'fillna'"
     ]
    }
   ],
   "source": [
    "params.update({\n",
    "    \"mean_probability_score\": {\n",
    "        \"fillna\": True,\n",
    "        \"square_usage\": True,\n",
    "        \"norm\": \"z\"\n",
    "    }\n",
    "})\n",
    "step_params = params[\"mean_probability_score\"]\n",
    "mean_probability_score = get_weighted_final_scores(\n",
    "    p_scores_df,\n",
    "    domain_usage_proportion,\n",
    "    square_usage=step_params[\"square_usage\"],\n",
    "    fillna=step_params[\"fillna\"]).T.sum().to_frame()\n",
    "proportions_used = domain_usage_proportion[p_scores_df.columns]\n",
    "if params[\"mean_probability_score\"][\"square_usage\"]:\n",
    "    proportions_used = proportions_used**2\n",
    "mean_probability_score /= proportions_used.T.sum().to_frame()\n",
    "mean_scores_scaler = z_normalize_by_all(mean_probability_score,\n",
    "                                        train_devices,\n",
    "                                        per_column=False)\n",
    "# change this to sum,then divide by sum of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for idx, group in device_targets.set_index(\"Device_ID\").join(mean_probability_score).set_index(\n",
    "        \"Target\").groupby(\"Target\"):\n",
    "    plt.hist(group, bins=100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "weighted_final_scores.join(device_targets.set_index(\"Device_ID\")).set_index(\n",
    "    \"Target\").stack().groupby(\"Target\").hist(bins=100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "mean_probability_score.join(device_targets.set_index(\"Device_ID\")).set_index(\n",
    "    \"Target\").stack().groupby(\"Target\").hist(bins=100, alpha=0.5, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"max_domain_scaler\": max_domain_scaler,\n",
    "    \"psd_df_scaler\": psd_df_scaler_params,\n",
    "    \"domains_visited_scaler\": domains_visited_scaler_params,\n",
    "    \"cls_proportion_scaler\": cls_proportion_scaler,\n",
    "    \"weighted_score_scaler\": scores_scaler,\n",
    "    \"mean_scores_scaler\": mean_scores_scaler\n",
    "}\n",
    "for k, v in scalers.items():\n",
    "    with open(f\"submission/{k}.json\", 'w') as f:\n",
    "        json.dump(v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/prepare_and_train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/prepare_and_train_model.py\n",
    "# model training\n",
    "import xgboost\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def join_features(device_targets, weighted_final_scores=None, domain_usage_proportion=None, \n",
    "                  cls_proportion=None, psd_df=None,domains_visited_proportion=None,mean_probability_score=None,max_domain_usage=None):\n",
    "    final_features = device_targets.set_index(\"Device_ID\") if device_targets is not None else None\n",
    "\n",
    "    if weighted_final_scores is not None and final_features is not None:\n",
    "        final_features = final_features.join(weighted_final_scores.rename(columns = lambda x: \"p_\"+str(x)), how=\"left\")\n",
    "        print(\"weighted_final_scores\")\n",
    "        print(weighted_final_scores.stack().describe())\n",
    "    else:\n",
    "        final_features = weighted_final_scores.rename(columns = lambda x: \"p_\"+str(x))\n",
    "        \n",
    "    if domain_usage_proportion is not None:  \n",
    "        final_features = final_features.join(domain_usage_proportion.rename(columns = lambda x: \"domain_usage_\"+str(x)), how=\"left\")\n",
    "        print(\"domain_usage_proportion\")\n",
    "        print(domain_usage_proportion.stack().describe())\n",
    "    if cls_proportion is not None:\n",
    "        final_features = final_features.join(cls_proportion.rename(columns = lambda x: \"cls_proportion_\"+str(x)), how=\"left\")\n",
    "        print(\"cls_proportion\")\n",
    "        print(cls_proportion.stack().describe())\n",
    "    if psd_df is not None:\n",
    "        final_features = final_features.join(psd_df.rename(columns = lambda x: \"activity_ps_\"+str(x)), how=\"left\")\n",
    "        print(\"psd_df\")\n",
    "        print(psd_df.stack().describe())\n",
    "    if domains_visited_proportion is not None:\n",
    "        final_features = final_features.join(domains_visited_proportion.rename(columns = lambda x: \"domains_visited_\"+str(x)), how=\"left\")\n",
    "        print(\"domains_visited_proportion\")\n",
    "        print(domains_visited_proportion.stack().describe())\n",
    "    if mean_probability_score is not None:\n",
    "        final_features = final_features.join(mean_probability_score.rename(columns = lambda x: \"mean_p_\"+str(x)), how=\"left\")\n",
    "        print(\"mean_probability_score\")\n",
    "        print(mean_probability_score.stack().describe())\n",
    "    if max_domain_usage is not None:\n",
    "        final_features = final_features.join(max_domain_usage.rename(columns = lambda x: \"max_domain_usage_\"+str(x)), how=\"left\")\n",
    "        print(\"max_domain_usage\")\n",
    "        print(max_domain_usage.stack().describe())\n",
    "    # final_features = final_features.join(device_domain_PCA,how=\"left\")\n",
    "    # if active_days is not None:\n",
    "    #     final_features = final_features.join(active_days,how=\"left\")\n",
    "    # if activity_per_time_range is not None:\n",
    "    #     final_features = final_features.join(activity_per_time_range,how=\"left\")\n",
    "    # final_features = final_features.fillna(0)\n",
    "    final_features.columns = [str(col) for col in final_features.columns]\n",
    "    return final_features\n",
    "\n",
    "\n",
    "def prepare_model_data(final_features, train_devices, test_device_ids):\n",
    "    X_train = final_features[final_features.index.isin(\n",
    "        train_devices)].drop('Target', axis=1)\n",
    "    y_train = final_features[final_features.index.isin(\n",
    "        train_devices)]['Target']\n",
    "\n",
    "    X_test = final_features[final_features.index.isin(\n",
    "        test_device_ids)].drop('Target', axis=1)\n",
    "    y_test = final_features[final_features.index.isin(\n",
    "        test_device_ids)]['Target']\n",
    "    return X_train, y_train, X_test[X_train.columns], y_test\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_test=None, y_test=None, params=None):\n",
    "    deval = xgboost.DMatrix(X_test, y_test) if X_test is not None else None\n",
    "\n",
    "    xgb_reg = xgboost.XGBRegressor(**params[\"model\"], eval_metric=roc_auc_score)#,early_stopping_rounds=25)\n",
    "    selector = RFE(xgb_reg, **params[\"feature_selection\"], verbose=1)\n",
    "    selector = selector.fit(X_train, y_train)#, eval_set=[(X_train,y_train),(X_test, y_test)], verbose=1)\n",
    "    best_features = list(X_train.columns[selector.support_])\n",
    "    if X_test is not None:\n",
    "        test_prediction = selector.estimator_.predict(X_test[best_features])\n",
    "        test_auc = round(roc_auc_score(y_test, test_prediction), 3)\n",
    "        return test_auc, selector, best_features, y_test, test_prediction\n",
    "    return None, selector, best_features, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_final_scores\n",
      "count    6.269080e+07\n",
      "mean    -5.682810e-06\n",
      "std      2.069110e-01\n",
      "min     -7.476524e+01\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.057925e+02\n",
      "dtype: float64\n",
      "cls_proportion\n",
      "count    1.015410e+06\n",
      "mean     2.536213e-03\n",
      "std      1.446344e+00\n",
      "min     -1.950436e+00\n",
      "25%     -1.667260e-01\n",
      "50%     -7.209489e-02\n",
      "75%     -3.243260e-02\n",
      "max      9.684832e+02\n",
      "dtype: float64\n",
      "psd_df\n",
      "count    523090.000000\n",
      "mean         -0.003653\n",
      "std           0.939442\n",
      "min          -0.553277\n",
      "25%          -0.259755\n",
      "50%          -0.157254\n",
      "75%          -0.030918\n",
      "max          68.944162\n",
      "dtype: float64\n",
      "domains_visited_proportion\n",
      "count    6154.000000\n",
      "mean       -0.006521\n",
      "std         0.998039\n",
      "min        -2.000097\n",
      "25%        -0.751405\n",
      "50%        -0.062312\n",
      "75%         0.631406\n",
      "max         4.779837\n",
      "dtype: float64\n",
      "mean_probability_score\n",
      "count    6154.000000\n",
      "mean       -0.007108\n",
      "std         1.001939\n",
      "min        -8.567538\n",
      "25%        -0.799263\n",
      "50%        -0.088344\n",
      "75%         0.617424\n",
      "max        13.739008\n",
      "dtype: float64\n",
      "max_domain_usage\n",
      "count    6154.000000\n",
      "mean        0.000134\n",
      "std         1.000366\n",
      "min        -1.715224\n",
      "25%        -0.725972\n",
      "50%        -0.201463\n",
      "75%         0.494286\n",
      "max         6.533576\n",
      "dtype: float64\n",
      "data with a lot of zeros shouldnt be normalized with zscore\n"
     ]
    }
   ],
   "source": [
    "final_features = join_features(\n",
    "    device_targets=device_targets,\n",
    "    weighted_final_scores=weighted_final_scores,\n",
    "    cls_proportion=cls_proportion,\n",
    "    psd_df=psd_df,\n",
    "    domains_visited_proportion=domains_visited_proportion,\n",
    "    mean_probability_score=mean_probability_score,\n",
    "    max_domain_usage=max_domain_usage)\n",
    "X_train, y_train, X_test, y_test = prepare_model_data(final_features,\n",
    "                                                      train_devices,\n",
    "                                                      test_device_ids)\n",
    "print(\"data with a lot of zeros shouldnt be normalized with zscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/tom.touati/web-segmentation/e/WEB-128\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import neptune\n",
    "\n",
    "params.update({\n",
    "    \"feature_selection\": {\n",
    "        \"n_features_to_select\": 2000,\n",
    "        \"step\": 20000\n",
    "    },\n",
    "    \"model\":\n",
    "    dict(seed=0,\n",
    "         subsample=0.8,\n",
    "         colsample_bytree=0.6,\n",
    "         learning_rate=0.1,\n",
    "         n_estimators=250,\n",
    "         max_depth=6,\n",
    "         objective='binary:logistic'),  # ,reg_lambda = 1.3 ),\n",
    "    \"no-normalization-finalscores\":\n",
    "    True\n",
    "})\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"tom.touati/web-segmentation\",  # replace with your project\n",
    "    api_token=os.environ[\"NEPTUNE_API_TOKEN\"],\n",
    "    # name=\"Activity-Based Features\",\n",
    "    capture_stdout=True,\n",
    "    capture_stderr=True,\n",
    "    capture_hardware_metrics=True,\n",
    "    tags=[\"time-based-models\", \"activity-based-features\"],\n",
    "    description=\"User activity patterns analysis\",\n",
    "    mode=NEPTUNE_MODE)\n",
    "run[\"parameters\"] = params\n",
    "run[\"notebook\"].upload(\n",
    "    \"/home/tom.touati/mafat-challenge/code/time_based_model.ipynb\")\n",
    "\n",
    "score, selector, best_features, y_test, test_prediction = train_model(\n",
    "    X_train, y_train, X_test, y_test, params)\n",
    "print(f'The auc for validation set: {score}')\n",
    "# calculate confusion matrix relative to prediction\n",
    "\n",
    "run[\"metrics/roc_auc\"] = score\n",
    "run[\"metrics/selected_features\"] = str(best_features)\n",
    "run[\"metrics/feature_importances\"] = str(\n",
    "    selector.estimator_.feature_importances_)\n",
    "run[\"metrics/feature_ranking\"] = str(selector.ranking_)\n",
    "run[\"metrics/feature_support\"] = str(selector.support_)\n",
    "# # Close the run\n",
    "with open(\"submission/best_features.json\", \"w\") as fp:\n",
    "    json.dump(best_features, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain_selected = xgb.DMatrix(X_train[best_features], y_train)\n",
    "dtest_selected = xgb.DMatrix(X_test[best_features], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_full = xgb.DMatrix(X_train, y_train)\n",
    "dtest_full = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#use truncatedsvd to reduce the number of features\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "#fit on the training data\n",
    "svd.fit(X_train)\n",
    "#transform the training data\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_svd_full = xgb.DMatrix(X_train_svd, y_train)\n",
    "dtest_svd_full = xgb.DMatrix(X_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = dtrain_svd_full\n",
    "dtest = dtest_svd_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train model with early stopping using best features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create DMatrix objects for XGBoost\n",
    "\n",
    "params = {\n",
    "    'seed': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',  # Use AUC as evaluation metric,\n",
    "    # 'lambda': 1.3,\n",
    "    # 'alpha': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "# @ray.remote\n",
    "def train_model(X_train, y_train, X_test=None, y_test=None, params=None):\n",
    "    # Training parameters\n",
    "\n",
    "    # # Train with early stopping\n",
    "    evals_result = {}\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "        early_stopping_rounds=100,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    results = evals_result\n",
    "    return results, model\n",
    "\n",
    "\n",
    "def plot_auc(results, model):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['train']['auc'], label='Training AUC')\n",
    "    plt.plot(results['eval']['auc'], label='Validation AUC')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('ROC AUC Score')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # # Print final scores\n",
    "    print(f\"Best iteration: {model.best_iteration}\")\n",
    "    print(\n",
    "        f\"Best training AUC: {results['train']['auc'][model.best_iteration]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Best validation AUC: {results['eval']['auc'][model.best_iteration]:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "ress.append(train_model(X_train, y_train, X_test, y_test, params))\n",
    "\n",
    "results, model = train_model(X_train, y_train, X_test, y_test, params)\n",
    "\n",
    "# Plot training history\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(results['train']['auc'], label='Training AUC')\n",
    "# plt.plot(results['eval']['auc'], label='Validation AUC')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('ROC AUC Score')\n",
    "# plt.title('Training History')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Print final scores\n",
    "# print(f\"Best iteration: {model.best_iteration}\")\n",
    "# print(\n",
    "#     f\"Best training AUC: {results['train']['auc'][model.best_iteration]:.4f}\")\n",
    "# print(\n",
    "#     f\"Best validation AUC: {results['eval']['auc'][model.best_iteration]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(results, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with binary classification\n",
    "predictions_binary = test_prediction > 0.5\n",
    "\n",
    "# Create masks for correct and incorrect predictions\n",
    "correct_mask = predictions_binary == y_test\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "# Get feature columns with domains_visited_ prefix\n",
    "domains_visited_cols = [\n",
    "    col for col in final_features.columns\n",
    "    if col.startswith('domains_visited_n_domains')\n",
    "]\n",
    "\n",
    "# Create separate histograms for correct and incorrect predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histograms of usage of domains_visited_n_domains for correct and incorrect predictions, to see if there is a difference\n",
    "for mask, label in zip([correct_mask, incorrect_mask],\n",
    "                       ['Correct', 'Incorrect']):\n",
    "    X_test.loc[mask, domains_visited_cols].stack().hist(bins=100,\n",
    "                                                        alpha=0.5,\n",
    "                                                        label=label,\n",
    "                                                        density=True)\n",
    "plt.title('Distribution of Mean Domain Visit Proportions by Target Class')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Mean Domain Visit Proportion')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print mean differences\n",
    "means_by_target = final_features[[\"domains_visited_n_domains\"]].join(\n",
    "    device_targets.set_index(\"Device_ID\")).groupby(\n",
    "        \"Target\")[\"domains_visited_n_domains\"].mean()\n",
    "mean_diff = means_by_target.loc[1] - means_by_target.loc[0]\n",
    "print(f\"Mean difference (Target 1 - Target 0): {mean_diff.mean():.3f}\")\n",
    "\n",
    "run[\"plots/domain_visit_means\"].upload(\n",
    "    neptune.types.File.as_image(plt.figure()))\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create test size increments\n",
    "step = 50\n",
    "max_size = len(y_test)\n",
    "idx = np.arange(max_size)\n",
    "auc_scores_all = []\n",
    "for _ in range(10):\n",
    "    np.random.shuffle(idx)\n",
    "    y_check = y_test[idx]\n",
    "    prediction_check = test_prediction[idx]\n",
    "    sizes = np.arange(200, max_size, step)\n",
    "    auc_scores = []\n",
    "\n",
    "    # Calculate AUC for each size\n",
    "    for size in sizes:\n",
    "        auc = roc_auc_score(y_check[:size], prediction_check[:size])\n",
    "        auc_scores.append(auc)\n",
    "    auc_scores_all.append(auc_scores)\n",
    "    plt.plot(sizes, auc_scores, 'b-', alpha=1)\n",
    "# Calculate mean and std across runs\n",
    "auc_scores_mean = np.mean(auc_scores_all, axis=0)\n",
    "auc_scores_std = np.std(auc_scores_all, axis=0)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "\n",
    "plt.axhline(y=score, color='r', linestyle='--',\n",
    "            label=f'Final AUC: {score:.3f}')\n",
    "plt.plot(sizes, auc_scores_mean, 'b-', alpha=1)\n",
    "\n",
    "plt.title('AUC Score vs Test Set Size')\n",
    "plt.xlabel('Test Set Size')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f'Mean AUC: {np.mean(auc_scores):.3f}')\n",
    "print(f'Std AUC: {np.std(auc_scores):.3f}')\n",
    "# Calculate confidence intervals\n",
    "ci_upper = np.array(auc_scores) + 1.96 * np.std(auc_scores)\n",
    "ci_lower = np.array(auc_scores) - 1.96 * np.std(auc_scores)\n",
    "\n",
    "# Upload plot to neptune run\n",
    "# run[\"plots/auc_vs_size\"].upload(plt.gcf())\n",
    "# run.stop()\n",
    "# Print additional statistics\n",
    "print(f'95% CI: [{np.mean(auc_scores)-1.96*np.std(auc_scores):.3f}, {np.mean(auc_scores)+1.96*np.std(auc_scores):.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training AUC-ROC history\n",
    "# evals_result = selector.estimator_.evals_result()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(len(evals_result['validation_0']['auc'])),\n",
    "#          evals_result['validation_0']['auc'],\n",
    "#          'b-', label='Training AUC')\n",
    "\n",
    "# plt.title('AUC-ROC Score Timeline')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('AUC-ROC Score')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Print final AUC\n",
    "# print(f'Final AUC-ROC Score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_train_pred = selector.estimator_.predict(X_train[best_features])\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred > 0.5))\n",
    "print(accuracy_score(y_test, test_prediction > 0.5))\n",
    "cm = confusion_matrix(y_test, test_prediction > 0.5, normalize='true')\n",
    "\n",
    "# create confusion matrix heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Create heatmap with percentage values\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Confusion Matrix (% of true labels)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate accuracy for different thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "accuracies = [accuracy_score(y_test, test_prediction > t) for t in thresholds]\n",
    "\n",
    "# Plot accuracy vs threshold curve\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "\n",
    "# Add vertical line at 0.5 threshold\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='0.5 threshold')\n",
    "\n",
    "# Add horizontal line at max accuracy\n",
    "max_accuracy = max(accuracies)\n",
    "plt.axhline(y=max_accuracy, color='g', linestyle='--',\n",
    "            label=f'Max accuracy: {max_accuracy:.3f}')\n",
    "\n",
    "plt.title('Model Performance on Validation Set')\n",
    "plt.xlabel('Prediction Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print additional metrics\n",
    "optimal_threshold = thresholds[np.argmax(accuracies)]\n",
    "print(f'Optimal threshold: {optimal_threshold:.3f}')\n",
    "print(f'ROC AUC score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_train_and_save(final_features):\n",
    "    final_x = final_features.drop(\"Target\", axis=1)\n",
    "    final_y = final_features[\"Target\"]\n",
    "    full_data_model, full_data_features = train_model(\n",
    "        final_x, final_y, params=params[\"model\"])\n",
    "    import json\n",
    "    with open(\"submission/best_features.json\", \"w\") as fp:\n",
    "        json.dump(full_data_features, fp)\n",
    "    import os\n",
    "    os.makedirs(\"submission\", exist_ok=True)\n",
    "    # with open(\"submission/XGB_model.json\", \"w\") as fp:\n",
    "    full_data_model.estimator_.save_model('submission/XGB_model.json')\n",
    "    return full_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "# with open(\"submission/XGB_model.json\", \"w\") as fp:\n",
    "selector.estimator_.save_model('submission/XGB_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import json\n",
    "from domain_timeseries_processing import *\n",
    "from utils import *\n",
    "from load_and_prepare_input import *\n",
    "from prepare_and_train_model import *\n",
    "from content_based_features import *\n",
    "from frequency_base_feats import *\n",
    "from cls_features import *\n",
    "\n",
    "\n",
    "class model:\n",
    "\n",
    "    def __init__(self, with_neptune=False):\n",
    "        '''\n",
    "        Init the model\n",
    "        '''\n",
    "        self.model = xgboost.XGBRegressor()\n",
    "        self.domain_activity = None\n",
    "        self.user_activity = None\n",
    "        self.best_features = None\n",
    "        self.weighted_score_scaler = StandardScaler(feature_range=(-1, 1))\n",
    "        self.max_domain_scaler = StandardScaler()\n",
    "        self.cls_proportion_scaler = StandardScaler()\n",
    "        self.domains_visited_scaler = StandardScaler()\n",
    "        self.psd_df_scaler = StandardScaler()\n",
    "        self.mean_scores_scaler = StandardScaler()\n",
    "    def get_probability_score(self, x_domains):\n",
    "        user_activity_timeseries = get_user_activity_timeseries(\n",
    "            x_domains, self.params[\"user_activity_timeseries\"])\n",
    "\n",
    "        p_scores_df = get_user_domain_scores(self.domain_activity,\n",
    "                                             user_activity_timeseries)\n",
    "        p_scores_df -= 0.5\n",
    "        # min_max_scale_all_values(p_scores_df, train_devices, self.score_scaler)\n",
    "        return p_scores_df\n",
    "\n",
    "    def get_cls_features(self, cls_data):\n",
    "        cls_proportion = get_cls_proportion(cls_data)\n",
    "        z_normalize_by_all(\n",
    "            cls_proportion, train_devices, per_column=True,fill_na_pre_transform=False,\n",
    "            scaler=self.cls_proportion_scaler)\n",
    "        return cls_proportion\n",
    "\n",
    "    def get_content_based_features(self, x_domains):\n",
    "        domain_usage_proportion = get_domain_usage_proportion(x_domains)\n",
    "        max_domain_usage = domain_usage_proportion.max(axis=1).to_frame()\n",
    "        z_normalize_by_all(df=max_domain_usage,\n",
    "                           train_devices=None,\n",
    "                           per_column=True,\n",
    "                           scaler=self.max_domain_scaler)\n",
    "        domain_usage_proportion = np.log(1 + domain_usage_proportion)\n",
    "        domain_usage_proportion = ((domain_usage_proportion.T) /\n",
    "                                   domain_usage_proportion.T.max()).T\n",
    "        return domain_usage_proportion, max_domain_usage\n",
    "\n",
    "    def get_frequency_based_features(self, db_df):\n",
    "        psd_df = get_ps_df(db_df)\n",
    "        z_normalize_by_all(\n",
    "            psd_df, train_devices, per_column=True,scaler=self.psd_df_scaler)\n",
    "        domains_visited_proportion = get_proportion_of_domains_visited(\n",
    "            db_df[db_df[\"Domain_Name\"].isin(devices_per_valid_domain.index)])\n",
    "        z_normalize_by_all(domains_visited_proportion,\n",
    "                           train_devices,\n",
    "                           per_column=True,\n",
    "                           fillval=0,\n",
    "                           scaler=self.domains_visited_scaler)\n",
    "        return psd_df, domains_visited_proportion\n",
    "\n",
    "    def get_mixed_features(self, p_scores_df, domain_usage_proportion):\n",
    "        weighted_final_scores = get_weighted_final_scores(\n",
    "            p_scores_df, domain_usage_proportion)\n",
    "        z_normalize_by_all(\n",
    "        weighted_final_scores, train_devices, per_column=False,fill_na_pre_transform=False,\n",
    "        scaler=self.weighted_score_scaler)\n",
    "\n",
    "        # add mean and std of weighted_final_scores\n",
    "        return weighted_final_scores, weighted_final_scores.T.mean(\n",
    "        ), weighted_final_scores.T.std()\n",
    "\n",
    "    def load_standard_scaler(self, scaler_path):\n",
    "        '''\n",
    "        Load the StandardScaler from the given path\n",
    "        '''\n",
    "        with open(scaler_path, 'r') as f:\n",
    "            loaded_params = json.load(f)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.mean_ = np.array([loaded_params[\"mean_\"]], dtype=np.float64)\n",
    "        scaler.var_ = np.array([loaded_params[\"var_\"]], dtype=np.float64)\n",
    "        scaler.scale_ = np.array([loaded_params[\"scale_\"]], dtype=np.float64)\n",
    "        scaler.n_samples_seen_ = np.array([loaded_params[\"n_samples_seen_\"]],\n",
    "                                          dtype=np.int64)\n",
    "        return scaler\n",
    "\n",
    "    def load_minmax_scaler(self, scaler_path):\n",
    "        '''\n",
    "        Load the MinMaxScaler from the given path\n",
    "        '''\n",
    "        with open(scaler_path, 'r') as f:\n",
    "            loaded_params = json.load(f)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.min_ = np.array([loaded_params[\"min_\"]], dtype=np.float64)\n",
    "        scaler.scale_ = np.array([loaded_params[\"scale_\"]], dtype=np.float64)\n",
    "        scaler.data_min_ = np.array([loaded_params[\"data_min_\"]],\n",
    "                                    dtype=np.float64)\n",
    "        scaler.data_max_ = np.array([loaded_params[\"data_max_\"]],\n",
    "                                    dtype=np.float64)\n",
    "        scaler.data_range_ = np.array([loaded_params[\"data_range_\"]],\n",
    "                                      dtype=np.float64)\n",
    "        scaler.feature_range = tuple(loaded_params[\"feature_range\"])\n",
    "        return scaler\n",
    "\n",
    "    def load(self, dir_path):\n",
    "        '''\n",
    "        Load the trained model and domain activity data\n",
    "        '''\n",
    "        import os\n",
    "        import json\n",
    "\n",
    "        model_path = os.path.join(dir_path, 'XGB_model.json')\n",
    "        self.model.load_model(model_path)\n",
    "        best_features_path = os.path.join(dir_path, 'best_features.json')\n",
    "        with open(best_features_path, \"r\") as fp:\n",
    "            self.best_features = json.load(fp)\n",
    "        self.best_domains = [x for x in self.best_features if x.isnumeric()]\n",
    "        domain_activity_path = os.path.join(dir_path,\n",
    "                                            'best_domain_activity.parquet')\n",
    "        self.domain_activity = pd.read_parquet(domain_activity_path)\n",
    "        # self.load_minmax_scaler(os.path.join(dir_path, 'minmax_scaler.json'))\n",
    "        f\n",
    "\n",
    "    def prepare_data(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if X['Datetime'].dtype == 'O':\n",
    "            X['Datetime'] = pd.to_datetime(X['Datetime'])\n",
    "        if \"Device_ID\" not in X.columns:\n",
    "            X[\"Device_ID\"] = 1\n",
    "        X.set_index(['Datetime'], inplace=True)\n",
    "        x_domains = X[X['Domain_Name'].isin(\n",
    "            [int(x) for x in self.best_domains])]\n",
    "        return X, x_domains\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class probability for the input data\n",
    "        '''\n",
    "        # Process user timeseries\n",
    "        X, x_best_domains = self.prepare_data(X)\n",
    "        p_scores_df = self.get_probability_score(x_best_domains)\n",
    "        domain_usage_proportion, max_domain_usage = self.get_content_based_features(\n",
    "            x_best_domains)\n",
    "        cls_proportion = self.get_cls_features(X)\n",
    "        psd_df, domains_visited_proportion = self.get_frequency_based_features(\n",
    "            X)\n",
    "        weighted_final_scores, mean_probability_score, std_probability_score = self.get_mixed_features(\n",
    "            p_scores_df, domain_usage_proportion)\n",
    "        final_features = join_features(\n",
    "            device_targets=None,\n",
    "            weighted_final_scores=weighted_final_scores,\n",
    "            cls_proportion=cls_proportion,\n",
    "            psd_df=psd_df,\n",
    "            domains_visited_proportion=domains_visited_proportion,\n",
    "            mean_probability_score=mean_probability_score,\n",
    "            max_domain_usage=max_domain_usage)\n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(final_features)\n",
    "        return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = db_df[db_df[\"Device_ID\"].isin(test_device_ids)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/tom.touati/mafat-challenge/code/submission/model.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
