{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fresh Idea\n",
    "## separate one/zero activity of domains\n",
    "- replace zeros by minus one\n",
    "- calculate the class activity for 3 hours bins for each domain\n",
    "- calculate the user activity for gaussian around center of 3 hour bins\n",
    "- calculate the likelihood of the person being a 1/-1 in that time \n",
    "- add user general metrics including domain cls activity and usage patterns\n",
    "\n",
    "### questions\n",
    "- how to take into account times when the person used a website when others didnt?\n",
    "- how to give likelihood when the person didn't show any nearby activity?\n",
    "- what if he used a similar website at same time but more nich? \n",
    "- how to average the bins weighted by the significance of that bin?\n",
    "- how to give weight to the magnitude of number of users entering? probability of 1 with confidence\n",
    "- what about sparse websites?\n",
    "- how to not let times where there are no activity take a lot of weight?\n",
    "### enhancements\n",
    "- create graph embedding of urls\n",
    "- for each bin, calculate the metric per url\n",
    "- instead of only looking at the specific website, take into account websites with similar usages,\n",
    "  for example looking at same domain_cls usage in gaussian around bin, or looking at domain embeddings and looking at the activity in similar embeddings weighted by the distance in the embedding space\n",
    "\n",
    "### NOTICE:\n",
    "the data itself will use all domains, even ones that the person never used. this could be an issue. \n",
    "first of all the fact that the person doesnt use them is an indication. we \n",
    "- we might want to take the niche websites and sum them up\n",
    "- we might want to remove them\n",
    "\n",
    "IDEA!\n",
    "- use different features for different people\n",
    "- make an ensemble that can differentiate between different users\n",
    "- take the people that get a wrong prediction and see if a classifier that is more \"fringe\" can classify them better\n",
    "- for example another tree classifier that takes a smaller amount of features to give more opportunity to fringe websites\n",
    "can create a classifier for each user type \n",
    "can take number of usages for each domain, and cluster people or PCA\n",
    "clustering is good - I can create a classifier for each cluster, from each cluster take all of the available data for all of the visited domains, and create a classifier for them. use only data from those users or all users that used one of the websites, plus the general model, for each cluster - use cluster model and general model.\n",
    "also - I can multiply the features by the log of usages\n",
    "\n",
    "another idea is to simply average the most prominent websites weighted by the specific user usages, and the general usage\n",
    "\n",
    "USER CLUSTER AS FEATURE - or PCA coefficients\n",
    "\n",
    "an idea - see how chaotic are the subject's patterns, if it's too predictable then it might be a bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "### low activity enhancement:\n",
    "- use selected features as initial centroids for domain clustering (spherical kmeans with initial centroids)\n",
    "- impute selected features only for these people\n",
    "### using highly visited websites:\n",
    "- filter urls that more than 3-5 people visited\n",
    "- simple option: give a score per website which is baysian probability\n",
    "- deeper option : take all urls of highly visited websites\n",
    "    - take 12-24 hour bits - and connect a trail from user to timebin to 1/0, then to all urls, where it increments the edge score by 1\n",
    "    - get embedding (32-64)\n",
    "    - use umap to turn to manifold with 8-16 dims\n",
    "    - cluster the urls using kmeans or spectral clustering\n",
    "    - for each person, give score for each cluster, how many urls from each cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tom.touati/mafat-challenge/code/submission\")\n",
    "import importlib\n",
    "import os\n",
    "import ray\n",
    "from modin.config import NPartitions,RangePartitioning\n",
    "%matplotlib widget\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiMGMyZjIyZC0xMjQzLTQxNjQtYjZjZC0wMTRiZmJmZmRlZjYifQ==\"\n",
    "    # !export MODIN_CPUS=2\n",
    "# n_cpus = 8\n",
    "plasma_store_size = 170*(1024**3)\n",
    "heap_memory = 200*(1024**3)\n",
    "# os.environ[\"MODIN_CPUS\"] = str(n_cpus)\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "os.environ[\"MODIN_NPARTITIONS\"] = \"30\"\n",
    "os.environ[\"MODIN_RANGE_PARTITIONING\"] = \"True\"\n",
    "# os.environ[\"MODIN_MEMORY\"] = str(plasma_store_size)\n",
    "ray.init(num_cpus =30,ignore_reinit_error=True, object_store_memory=plasma_store_size,_memory=heap_memory)\n",
    "# print(ray.cluster_resources())\n",
    "import modin.pandas as mpd\n",
    "from modin import config as cfg\n",
    "print(vars(cfg))\n",
    "NEPTUNE_MODE=\"sync\"\n",
    "params={}\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/load_and_prepare_input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/load_and_prepare_input.py\n",
    "import os\n",
    "import sqlite3\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "\n",
    "\n",
    "# Modin will use Ray\n",
    "# ray.init()\n",
    "# NPartitions.put(16)\n",
    "def load_domain_data_from_db(con, domain_cls=False, only_domain=False):\n",
    "    try:\n",
    "        device_ids_query = f\"\"\"SELECT Datetime,Device_ID,Domain_Name,Target from data\n",
    "        WHERE Domain_Name != 1732927\n",
    "        \"\"\"\n",
    "\n",
    "        # WHERE Domain_Name != 1732927 \"\"\"\n",
    "        df = mpd.read_sql(device_ids_query, con)._repartition()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_cls_data_from_db(con, domain_cls=False, only_domain=False):\n",
    "    try:\n",
    "        device_ids_query = f\"\"\"SELECT Datetime,Device_ID,Domain_cls1,Domain_cls2,Domain_cls3,Domain_cls4,Target\n",
    "        from data\n",
    "        WHERE Domain_Name != 1732927\n",
    "        \"\"\"\n",
    "        df = mpd.read_sql(device_ids_query, con)._repartition()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_and_prepare_data(data_type=\"domain\"):\n",
    "    freeze_support()\n",
    "    dbfile = '../../data/training_set.db'\n",
    "\n",
    "    conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "    # Can use get_connection to get underlying sqlalchemy engine\n",
    "    conn.get_connection()\n",
    "    if data_type == \"domain\":\n",
    "        db_df = load_domain_data_from_db(conn)\n",
    "    elif data_type == \"cls\":\n",
    "        db_df = load_cls_data_from_db(conn)\n",
    "\n",
    "    print(db_df.head())\n",
    "    del conn\n",
    "    db_df['Datetime'] = mpd.to_datetime(db_df['Datetime'])\n",
    "    db_df.set_index('Datetime', inplace=True)\n",
    "    if data_type == \"domain\":\n",
    "        db_df = db_df.astype({\n",
    "            'Domain_Name': 'uint32',\n",
    "            'Device_ID': 'uint32',\n",
    "            'Target': 'uint8'\n",
    "        })\n",
    "    elif data_type == \"cls\":\n",
    "        db_df = db_df.astype({\n",
    "            'Device_ID': 'uint32',\n",
    "            'Domain_cls1': 'uint32',\n",
    "            'Domain_cls2': 'uint32',\n",
    "            'Domain_cls3': 'uint32',\n",
    "            'Domain_cls4': 'uint32',\n",
    "            'Target': 'uint32'\n",
    "        })\n",
    "    return db_df\n",
    "\n",
    "\n",
    "# prepare training data\n",
    "import matplotlib.pyplot as plt\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_test_devices(device_target_df, test_size=0.2, random_state=43):\n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target'])\n",
    "    return train_device_ids, test_device_ids\n",
    "\n",
    "\n",
    "def get_initial_train_data(db_df,\n",
    "                           test_size=0.2,\n",
    "                           random_state=42,\n",
    "                           min_domain_devices=10,\n",
    "                           n_devices_hist=False):\n",
    "    device_targets = db_df.groupby(\"Device_ID\")[\"Target\"].first().reset_index()\n",
    "    train_devices, test_device_ids = get_train_test_devices(\n",
    "        device_targets, test_size=test_size, random_state=random_state)\n",
    "    train_df = db_df[db_df[\"Device_ID\"].isin(train_devices)]\n",
    "    devices_per_domain = train_df.groupby(\"Domain_Name\")[\"Device_ID\"].nunique()\n",
    "\n",
    "    domain_mask = devices_per_domain > min_domain_devices\n",
    "    print(\n",
    "        f\"Percentage of domains with more than {min_domain_devices} devices: {domain_mask.mean()*100:.2f}%\"\n",
    "    )\n",
    "    devices_per_domain = devices_per_domain[domain_mask]\n",
    "    if n_devices_hist:\n",
    "        hist = devices_per_domain.hist()\n",
    "        # run[\"plots/domain_devices_hist\"].upload(neptune.types.File.as_image(hist.figure))\n",
    "        plt.show()\n",
    "    train_df = train_df[train_df[\"Domain_Name\"].isin(devices_per_domain.index)]\n",
    "    return train_df, train_devices, test_device_ids, device_targets, devices_per_domain\n",
    "\n",
    "\n",
    "# Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission.domain_timeseries_processing import *\n",
    "from submission.utils import *\n",
    "from submission.load_and_prepare_input import *\n",
    "from submission.prepare_and_train_model import *\n",
    "from submission.content_based_features import *\n",
    "from submission.frequency_base_feats import *\n",
    "from submission.cls_features import *\n",
    "\n",
    "url_df = load_and_prepare_data()\n",
    "\n",
    "params.update({\n",
    "    \"training_data\": {\n",
    "        \"min_domain_devices\": 10,\n",
    "        \"n_devices_hist\": False,\n",
    "        \"test_size\": 0.2,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "})\n",
    "train_df, train_devices, test_device_ids, device_targets, devices_per_valid_domain = get_initial_train_data(\n",
    "    url_df, **params[\"training_data\"])\n",
    "train_df = train_df._repartition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/utils.py\n",
    "import numpy as np\n",
    "import modin.pandas as mpd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "def z_normalize_by_all(df,\n",
    "                       train_devices,\n",
    "                       per_column=True,\n",
    "                       fillval=0,\n",
    "                       fill_na_pre_transform=False,\n",
    "                       scaler=None):\n",
    "    if scaler is not None:\n",
    "        if fill_na_pre_transform:\n",
    "            df.fillna(fillval, inplace=True)\n",
    "        df.iloc[:, :] = scaler.transform(\n",
    "            df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "        if fillval:\n",
    "            df.fillna(fillval, inplace=True)\n",
    "        return\n",
    "    scaler = StandardScaler()\n",
    "    train_data = df.loc[train_devices]\n",
    "    scaler.fit(train_data if per_column else train_data.values.reshape(-1, 1))\n",
    "\n",
    "    # Transform all data using fitted scaler\n",
    "    if fill_na_pre_transform:\n",
    "        df.fillna(fillval, inplace=True)\n",
    "    df.iloc[:, :] = scaler.transform(\n",
    "        df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "    if fillval is not None:\n",
    "        df.fillna(fillval, inplace=True)\n",
    "    params = {\n",
    "        \"mean_\":\n",
    "        [float(x)\n",
    "         for x in scaler.mean_],  # Convert to list for JSON serialization\n",
    "        \"var_\": [float(x) for x in scaler.var_],\n",
    "        \"scale_\": [float(x) for x in scaler.scale_],\n",
    "        \"n_samples_seen_\": [int(x) for x in scaler.n_samples_seen_]\n",
    "        if isinstance(scaler.n_samples_seen_, np.ndarray) else int(\n",
    "            scaler.n_samples_seen_),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def min_max_scale_all_values(df, train_devices, per_column=False, scaler=None):\n",
    "    if scaler is not None:\n",
    "        df.iloc[:, :] = scaler.transform(\n",
    "            df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "        return\n",
    "    # Create MinMaxScaler and fit on training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_data = df.loc[train_devices]\n",
    "    scaler.fit(train_data if per_column else train_data.values.reshape(-1, 1))\n",
    "\n",
    "    # Transform all data using fitted scaler\n",
    "    df.iloc[:, :] = scaler.transform(\n",
    "        df if per_column else df.values.reshape(-1, 1)).reshape(df.shape)\n",
    "\n",
    "    # Save scaler parameters as JSON\n",
    "    scaler_params = {\n",
    "        \"min_\": float(scaler.min_[0]),  # Convert to native Python float\n",
    "        \"scale_\": float(scaler.scale_[0]),\n",
    "        \"data_min_\": float(scaler.data_min_[0]),\n",
    "        \"data_max_\": float(scaler.data_max_[0]),\n",
    "        \"data_range_\": float(scaler.data_range_[0]),\n",
    "        # Convert tuple to list for JSON\n",
    "        \"feature_range\": list(scaler.feature_range)\n",
    "    }\n",
    "    return scaler_params\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer  # , IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# from implicit.cpu.als import AlternatingLeastSquares\n",
    "def impute_missing_values(final_scores_pivot, train_device_ids):\n",
    "    # Use SoftImpute to fill missing values\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    imputer.fit(final_scores_pivot.loc[train_device_ids])\n",
    "    imputed_scores = imputer.transform(final_scores_pivot)\n",
    "    # imputer = AlternatingLeastSquares(factors=10, regularization=0.01, iterations=10,random_state=0)\n",
    "    # # imputer = MissForest(max_depth=6,max_features=0.8, random_state=0)\n",
    "    # imputer.fit( user_items = csr_matrix(final_scores_pivot.loc[train_device_ids].values))\n",
    "\n",
    "    # imputed_scores = imputer.recommend_all(csr_matrix(final_scores_pivot.values))\n",
    "    return imputed_scores\n",
    "\n",
    "\n",
    "import gc\n",
    "import ctypes\n",
    "import sys\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Force cleanup of memory by:\n",
    "    1. Running garbage collection\n",
    "    2. Attempting to release memory back to OS\n",
    "    \"\"\"\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Attempt to release memory back to the OS\n",
    "    if sys.platform.startswith('linux'):\n",
    "        libc = ctypes.CDLL('libc.so.6')\n",
    "        # MALLOC_TRIM(0) releases memory back to OS if possible\n",
    "        print(libc.malloc_trim(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best features of cls and domain\n",
    "#load usage proportions for domains\n",
    "#use hdbscan with these initial centroids\n",
    "# get score for each cluster by weighing the usage proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
