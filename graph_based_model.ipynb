{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Device_ID                   Datetime      URL  Domain_Name  Domain_cls1  \\\n",
      "0        124  2023-04-23 03:04:30+03:00     6466      2368671          755   \n",
      "1        124  2023-04-23 03:04:30+03:00  2245864      1792903            0   \n",
      "2        124  2023-04-23 03:04:30+03:00  1839478       107342          332   \n",
      "3        124  2023-04-23 03:14:50+03:00  1172090       107342          332   \n",
      "4        124  2023-04-23 03:14:50+03:00  1839478       107342          332   \n",
      "\n",
      "   Domain_cls2  Domain_cls3  Domain_cls4  Target  \n",
      "0          799            0            0       0  \n",
      "1            0            0            0       0  \n",
      "2            0            0            0       0  \n",
      "3            0            0            0       0  \n",
      "4            0            0            0       0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from multiprocessing import freeze_support\n",
    "from modin.db_conn import ModinDatabaseConnection\n",
    "import modin.pandas as mpd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "def load_data_from_db(con):\n",
    "    try:\n",
    "        df = mpd.read_sql(\"SELECT * FROM data\", con)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "dbfile = '/home/tompouce/workspaces/mafat-challenge/train_data_for_competition/mini_training_set.db'\n",
    "\n",
    "conn = ModinDatabaseConnection('sqlalchemy', f'sqlite:///{dbfile}')\n",
    "\n",
    "# Can use get_connection to get underlying sqlalchemy engine\n",
    "conn.get_connection()\n",
    "db_df = load_data_from_db(conn)\n",
    "print(db_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2023-04-23 03:04:30+03:00\n",
       "1          2023-04-23 03:04:30+03:00\n",
       "2          2023-04-23 03:04:30+03:00\n",
       "3          2023-04-23 03:14:50+03:00\n",
       "4          2023-04-23 03:14:50+03:00\n",
       "                      ...           \n",
       "32248978   2023-05-13 21:26:02+03:00\n",
       "32248979   2023-05-13 21:26:03+03:00\n",
       "32248980   2023-05-13 21:32:33+03:00\n",
       "32248981   2023-05-13 21:32:38+03:00\n",
       "32248982   2023-05-13 21:32:43+03:00\n",
       "Name: Datetime, Length: 32248983, dtype: datetime64[ns, UTC+03:00]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"23-04 to 18-05\"\n",
    "db_df[\"Datetime\"] = mpd.to_datetime(db_df[\"Datetime\"])\n",
    "db_df[\"Datetime\"]\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x-x[\"Datetime\"].min()).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>Target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124</td>\n",
       "      <td>3930</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>4136</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>6450</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124</td>\n",
       "      <td>12837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>17665</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288003</th>\n",
       "      <td>69967</td>\n",
       "      <td>2361028</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288004</th>\n",
       "      <td>69967</td>\n",
       "      <td>2387835</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288005</th>\n",
       "      <td>69967</td>\n",
       "      <td>2389761</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288006</th>\n",
       "      <td>69967</td>\n",
       "      <td>2390487</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288007</th>\n",
       "      <td>69967</td>\n",
       "      <td>2408371</td>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288008 rows x 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source   target  Target  weight\n",
       "0          124     3930       0       4\n",
       "1          124     4136       0       3\n",
       "2          124     6450       0       3\n",
       "3          124    12837       0       1\n",
       "4          124    17665       0       2\n",
       "...        ...      ...     ...     ...\n",
       "288003   69967  2361028       1       4\n",
       "288004   69967  2387835       1      34\n",
       "288005   69967  2389761       1      72\n",
       "288006   69967  2390487       1       1\n",
       "288007   69967  2408371       1     385\n",
       "\n",
       "[288008 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_domain_counts(db_df, pivot=False):\n",
    "    domain_counts = db_df.groupby([\"Device_ID\",\"Domain_Name\",\"Target\"]).count()\n",
    "    domain_counts = domain_counts.reset_index()\n",
    "    domain_counts = domain_counts[[\"Device_ID\",\"Domain_Name\",\"Target\",\"Datetime\"]]\n",
    "    domain_counts.rename(columns={\"Datetime\":\"count\"}, inplace=True)\n",
    "    if pivot:\n",
    "        pivot_matrix = train_domain_counts.pivot(index='source', columns='target', values='count').fillna(0)\n",
    "        return pivot_matrix\n",
    "    return domain_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total devices: 615\n",
      "Train devices: 492\n",
      "Test devices: 123\n",
      "\n",
      "Train samples: 228881\n",
      "Test samples: 59127\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_masks(domain_counts, test_size=0.2, random_state=42):\n",
    "    # Get unique device IDs and their corresponding targets\n",
    "    device_target_df = domain_counts.groupby('Device_ID')['Target'].first().reset_index()\n",
    "    \n",
    "    # Perform stratified split on device IDs\n",
    "    train_device_ids, test_device_ids = train_test_split(\n",
    "        device_target_df['Device_ID'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=device_target_df['Target']\n",
    "    )\n",
    "    \n",
    "    # Create mask for train/test split in domain_counts\n",
    "    train_mask = domain_counts['Device_ID'].isin(train_device_ids)\n",
    "    test_mask = domain_counts['Device_ID'].isin(test_device_ids)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total devices: {len(device_target_df)}\")\n",
    "    print(f\"Train devices: {len(train_device_ids)}\")\n",
    "    print(f\"Test devices: {len(test_device_ids)}\")\n",
    "    print(f\"\\nTrain samples: {len(domain_counts[train_mask])}\")\n",
    "    print(f\"Test samples: {len(domain_counts[test_mask])}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nTarget distribution in train set:\")\n",
    "    print(domain_counts[train_mask].groupby('Target').size() / len(domain_counts[train_mask]))\n",
    "    print(\"\\nTarget distribution in test set:\")\n",
    "    print(domain_counts[test_mask].groupby('Target').size() / len(domain_counts[test_mask]))\n",
    "    \n",
    "    return train_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"keep in mind that the best resolution will be achieved with a resolution of urls/chain of urls, not domains\")\n",
    "print(\"Cluster url walks\")\n",
    "print(\"I want to cluster urls/url walks from a given domain, to 3 categories\")\n",
    "print(\"positive correlation, zero correlation, negative correlation, to_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "def compute_chunked_covariance(pivot_matrix, batch_size=2000):\n",
    "    # Convert to numpy array for faster computation\n",
    "    matrix_dense = pivot_matrix.to_numpy()\n",
    "    matrix_centered = matrix_dense - np.mean(matrix_dense, axis=0)\n",
    "\n",
    "    # Initialize parameters\n",
    "    n_cols = pivot_matrix.shape[1]\n",
    "    futures = []\n",
    "\n",
    "    # Submit tasks to Ray\n",
    "    for i in range(0, n_cols, batch_size):\n",
    "        batch_end = min(i + batch_size, n_cols)\n",
    "        futures.append(calculate_chunk_covariance.remote(matrix_centered, i, batch_end, pivot_matrix.columns))\n",
    "\n",
    "    # Collect results and combine\n",
    "    cov_chunks = []\n",
    "    for future in ray.get(futures):\n",
    "        start_idx, end_idx, chunk_cov = future\n",
    "        chunk_df = mpd.DataFrame(\n",
    "            chunk_cov,\n",
    "            index=pivot_matrix.columns[start_idx:end_idx],\n",
    "            columns=pivot_matrix.columns\n",
    "        )\n",
    "        cov_chunks.append(chunk_df)\n",
    "\n",
    "    # Combine all chunks\n",
    "    return mpd.concat(cov_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = cov_mat.iloc[:,:cov_mat.shape[1]//2]\n",
    "second_half = cov_mat.iloc[:,cov_mat.shape[1]//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quarter = first_half.iloc[:first_half.shape[1]//100,:first_half.shape[1]//100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.data as rd\n",
    "import ray\n",
    "# Convert Modin DataFrame to Ray Dataset\n",
    "# First get the Pandas partitions from Modin\n",
    "# Initialize Ray if not already initialized\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "# Convert the large Modin DataFrame to Ray Dataset in batches\n",
    "ds = rd.from_modin(cov_mat)\n",
    "\n",
    "# Write the dataset to parquet files with automatic batching\n",
    "ds.write_parquet(\n",
    "    'url_cov_mat_rays',\n",
    "    filesystem=None,  # Local filesystem\n",
    "    row_group_size_bytes=100_000_000,  # Adjust based on available memory (100MB)\n",
    "    compression='snappy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "n_cols = cov_mat.shape[1]\n",
    "cov_path = \"/home/tompouce/workspaces/mafat-challenge/eda/cov_matrices\"\n",
    "for i in range(0, n_cols, chunk_size):\n",
    "    end_idx = min(i + chunk_size, n_cols)\n",
    "    chunk = cov_mat.iloc[:, i:end_idx]\n",
    "    chunk_name = f'url_cov_mat_chunk_{i}_{end_idx}.parquet'\n",
    "    chunk.to_parquet(os.path.join(cov_path,chunk_name), engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cov_mat*100).astype(\"uint8\").to_parquet('url_cov_mat.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N2V = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=8)\n",
    "# model = N2V.fit(window=10, min_count=1, batch_words=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.save_word2vec_format(\"user2url_node2vec_embeddings.emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df.groupby(\"Domain_cls2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df.head()\n",
    "divide_ids = list(db_df[\"Device_ID\"].unique())\n",
    "print(len(divide_ids))\n",
    "regular_df = db_df[db_df[\"Device_ID\"].isin(divide_ids[:20])]\n",
    "# %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_df = regular_df._to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_df[\"count_weighted_by_days\"] = db_df.groupby(\"Device_ID\").apply(lambda x: (x[\"Datetime\"]-x[\"Datetime\"].min()).dt.days).astype(\"float32\")\n",
    "# db_df[\"count_weighted_by_days\"] = db_df[\"count_weighted_by_days\"] *0.1+1\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x[\"count_weighted_by_days\"]/x[\"count_weighted_by_days\"].sum())).astype(\"float64\")\n",
    "# db_df.groupby(\"Device_ID\").apply(lambda x: (x[\"count_weighted_by_days\"]/x[\"count_weighted_by_days\"].sum())).astype(\"float64\")\n",
    "\n",
    "# db_df.groupby(\"Device_ID\")[\"count_weighted_by_days\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
